{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56540397-c524-4678-8a18-2deac21f28ef",
   "metadata": {},
   "source": [
    "1st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdbd5bc2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'timestamp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'timestamp'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransaction.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Replace with your dataset path\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Data Preprocessing\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtimestamp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     16\u001b[0m df\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Aggregating the Footfall at Different Levels\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'timestamp'"
     ]
    }
   ],
   "source": [
    "# Required Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Load Dataset\n",
    "df = pd.read_csv('transaction.csv')  # Replace with your dataset path\n",
    "\n",
    "# Data Preprocessing\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Aggregating the Footfall at Different Levels\n",
    "daily_footfall = df.resample('D').count()  # Daily footfall\n",
    "weekly_footfall = df.resample('W').count()  # Weekly footfall\n",
    "monthly_footfall = df.resample('M').count()  # Monthly footfall\n",
    "\n",
    "# Exploratory Data Analysis (EDA) - Visualizing Trends\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(daily_footfall, label='Daily Footfall', color='blue')\n",
    "plt.title('Daily Metro Footfall Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Footfall')\n",
    "plt.legend()  # Adding legend\n",
    "plt.show()\n",
    "\n",
    "# Stationarity Check using Augmented Dickey-Fuller Test\n",
    "result = adfuller(daily_footfall['transaction_id'])\n",
    "print(f'ADF Statistic: {result[0]}')\n",
    "print(f'p-value: {result[1]}')\n",
    "\n",
    "# Time-Series Decomposition\n",
    "decomposition = seasonal_decompose(daily_footfall['transaction_id'], model='additive')\n",
    "fig = decomposition.plot()\n",
    "fig.suptitle('Time-Series Decomposition: Trend, Seasonal, and Residuals', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# ARIMA Model - Time-Series Forecasting\n",
    "model = ARIMA(daily_footfall['transaction_id'], order=(5,1,0))  # Adjust order based on ACF/PACF analysis\n",
    "model_fit = model.fit()\n",
    "forecast = model_fit.forecast(steps=30)  # Forecast the next 30 days\n",
    "\n",
    "# Plotting ARIMA Forecast\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(daily_footfall.index, daily_footfall['transaction_id'], label='Observed', color='blue')\n",
    "plt.plot(pd.date_range(daily_footfall.index[-1], periods=30, freq='D'), forecast, label='Forecast', color='orange')\n",
    "plt.title('ARIMA Model: Observed vs Forecasted Footfall')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Footfall')\n",
    "plt.legend()  # Adding legend\n",
    "plt.show()\n",
    "\n",
    "# Prophet Model - Alternative Time-Series Forecasting\n",
    "daily_footfall_prophet = daily_footfall.reset_index()\n",
    "daily_footfall_prophet.columns = ['ds', 'y']\n",
    "\n",
    "prophet_model = Prophet()\n",
    "prophet_model.fit(daily_footfall_prophet)\n",
    "future = prophet_model.make_future_dataframe(periods=365)  # Forecast 1 year into the future\n",
    "forecast_prophet = prophet_model.predict(future)\n",
    "\n",
    "# Plotting Prophet Forecast\n",
    "fig = prophet_model.plot(forecast_prophet)\n",
    "plt.title('Prophet Model Forecast')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Footfall')\n",
    "plt.legend(['Observed', 'Forecast'])  # Adding legend for Prophet model\n",
    "plt.show()\n",
    "\n",
    "# Evaluating the ARIMA Model\n",
    "# Assuming you have true values for evaluation\n",
    "# true_values = [place the actual values if available]\n",
    "# rmse = np.sqrt(mean_squared_error(true_values, forecast))\n",
    "# print(f'RMSE: {rmse}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2306dc7-95aa-4b2d-91fe-49cb1f6745d0",
   "metadata": {},
   "source": [
    "2nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c9e7e3-cab0-4f71-b682-3254b9149646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from pmdarima import auto_arima  # Use auto_arima for automatic ARIMA order selection\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Load Dataset\n",
    "df = pd.read_csv('transaction.csv')  # Replace with your dataset path\n",
    "\n",
    "# Data Preprocessing\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Aggregating the Footfall at Different Levels\n",
    "daily_footfall = df.resample('D').count()  # Daily footfall\n",
    "\n",
    "# Exploratory Data Analysis (EDA) - Visualizing Trends\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(daily_footfall, label='Daily Footfall', color='blue')\n",
    "plt.title('Daily Metro Footfall Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Footfall')\n",
    "plt.legend()  # Adding legend\n",
    "plt.show()\n",
    "\n",
    "# Stationarity Check using Augmented Dickey-Fuller Test\n",
    "result = adfuller(daily_footfall['transaction_id'])\n",
    "print(f'ADF Statistic: {result[0]}')\n",
    "print(f'p-value: {result[1]}')\n",
    "\n",
    "# Time-Series Decomposition\n",
    "decomposition = seasonal_decompose(daily_footfall['transaction_id'], model='additive')\n",
    "fig = decomposition.plot()\n",
    "fig.suptitle('Time-Series Decomposition: Trend, Seasonal, and Residuals', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Check the size of the dataset\n",
    "print(f\"Total dataset length: {len(daily_footfall)}\")\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "test_size = 30\n",
    "if len(daily_footfall) > test_size:\n",
    "    train_size = len(daily_footfall) - test_size\n",
    "    train, test = daily_footfall.iloc[:train_size], daily_footfall.iloc[train_size:]\n",
    "else:\n",
    "    train = daily_footfall\n",
    "    test = pd.DataFrame()  # Empty test set if not enough data\n",
    "\n",
    "# Check if train and test datasets are not empty\n",
    "print(f\"Train dataset length: {len(train)}\")\n",
    "print(f\"Test dataset length: {len(test)}\")\n",
    "\n",
    "if len(train) > 0 and len(test) > 0:\n",
    "    try:\n",
    "        # Automatically determine ARIMA order\n",
    "        auto_model = auto_arima(train['transaction_id'].dropna(), seasonal=False, stepwise=True)\n",
    "        print(\"Auto ARIMA Model Summary:\")\n",
    "        print(auto_model.summary())\n",
    "        \n",
    "        # Fit ARIMA model with the best parameters\n",
    "        arima_model = ARIMA(train['transaction_id'], order=auto_model.order)\n",
    "        arima_model_fit = arima_model.fit()\n",
    "        arima_forecast = arima_model_fit.forecast(steps=30)\n",
    "\n",
    "        # Evaluate ARIMA model\n",
    "        arima_rmse = np.sqrt(mean_squared_error(test['transaction_id'], arima_forecast))\n",
    "        arima_mae = mean_absolute_error(test['transaction_id'], arima_forecast)\n",
    "\n",
    "        # Display ARIMA Forecasted Data and Evaluation Metrics\n",
    "        print(\"ARIMA Model - 30 Day Forecasted Footfall:\")\n",
    "        print(arima_forecast)\n",
    "        print(f'ARIMA Model - RMSE: {arima_rmse}')\n",
    "        print(f'ARIMA Model - MAE: {arima_mae}')\n",
    "\n",
    "        # Plotting ARIMA Forecast\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(daily_footfall.index, daily_footfall['transaction_id'], label='Observed', color='blue')\n",
    "        plt.plot(pd.date_range(daily_footfall.index[-30], periods=30, freq='D'), arima_forecast, label='Forecast', color='orange')\n",
    "        plt.title('ARIMA Model: Observed vs Forecasted Footfall')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Footfall')\n",
    "        plt.legend()  # Adding legend\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred with ARIMA model: {e}\")\n",
    "else:\n",
    "    print(\"Not enough data for ARIMA model. Please check the data.\")\n",
    "\n",
    "# Prophet Model - Alternative Time-Series Forecasting\n",
    "daily_footfall_prophet = daily_footfall.reset_index()\n",
    "\n",
    "# Print columns to debug\n",
    "print(\"Columns before renaming:\", daily_footfall_prophet.columns)\n",
    "\n",
    "# Drop any additional columns and rename correctly\n",
    "if len(daily_footfall_prophet.columns) == 2:\n",
    "    daily_footfall_prophet.columns = ['ds', 'y']  # Rename for Prophet\n",
    "else:\n",
    "    raise ValueError(f\"Unexpected number of columns: {len(daily_footfall_prophet.columns)}\")\n",
    "\n",
    "# Ensure 'ds' is datetime and 'y' is numeric\n",
    "daily_footfall_prophet['ds'] = pd.to_datetime(daily_footfall_prophet['ds'])\n",
    "daily_footfall_prophet['y'] = pd.to_numeric(daily_footfall_prophet['y'], errors='coerce')\n",
    "\n",
    "# Fit Prophet model\n",
    "prophet_model = Prophet()\n",
    "prophet_model.fit(daily_footfall_prophet)\n",
    "future = prophet_model.make_future_dataframe(periods=365)  # Forecast 1 year into the future\n",
    "forecast_prophet = prophet_model.predict(future)\n",
    "\n",
    "# Evaluate Prophet model\n",
    "try:\n",
    "    # Extracting the last 30 days of forecast data for comparison\n",
    "    prophet_forecast_last_30 = forecast_prophet[['ds', 'yhat']].tail(30)\n",
    "    prophet_test = daily_footfall_prophet.tail(30).set_index('ds')\n",
    "\n",
    "    prophet_rmse = np.sqrt(mean_squared_error(prophet_test['y'], prophet_forecast_last_30['yhat']))\n",
    "    prophet_mae = mean_absolute_error(prophet_test['y'], prophet_forecast_last_30['yhat'])\n",
    "\n",
    "    # Display Prophet Forecasted Data and Evaluation Metrics\n",
    "    print(\"Prophet Model - 365 Day Forecasted Footfall:\")\n",
    "    print(forecast_prophet[['ds', 'yhat']].tail(30))  # Display last 30 days forecast\n",
    "    print(f'Prophet Model - RMSE: {prophet_rmse}')\n",
    "    print(f'Prophet Model - MAE: {prophet_mae}')\n",
    "\n",
    "    # Plotting Prophet Forecast\n",
    "    fig = prophet_model.plot(forecast_prophet)\n",
    "    plt.title('Prophet Model Forecast')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Footfall')\n",
    "    plt.legend(['Observed', 'Forecast'])  # Adding legend for Prophet model\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred with Prophet model: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6b7465-e157-4354-868f-bdfaef373018",
   "metadata": {},
   "source": [
    "3rd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeffeb6-1cf2-4e96-b47c-11655873e989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Load Dataset\n",
    "df = pd.read_csv('transaction.csv')  # Replace with your dataset path\n",
    "\n",
    "# Data Preprocessing\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Aggregating the Footfall at Different Levels\n",
    "daily_footfall = df.resample('D').count()  # Daily footfall\n",
    "\n",
    "# Exploratory Data Analysis (EDA) - Visualizing Trends\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(daily_footfall, label='Daily Footfall', color='blue')\n",
    "plt.title('Daily Metro Footfall Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Footfall')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Stationarity Check using Augmented Dickey-Fuller Test\n",
    "result = adfuller(daily_footfall['transaction_id'].dropna())\n",
    "print(f'ADF Statistic: {result[0]}')\n",
    "print(f'p-value: {result[1]}')\n",
    "if result[1] > 0.05:\n",
    "    print(\"The series is not stationary. Differencing may be required.\")\n",
    "\n",
    "# Time-Series Decomposition\n",
    "decomposition = seasonal_decompose(daily_footfall['transaction_id'].dropna(), model='additive')\n",
    "fig = decomposition.plot()\n",
    "fig.suptitle('Time-Series Decomposition: Trend, Seasonal, and Residuals', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "train_size = len(daily_footfall) - 30\n",
    "train, test = daily_footfall.iloc[:train_size], daily_footfall.iloc[train_size:]\n",
    "\n",
    "# ARIMA Model - Time-Series Forecasting\n",
    "try:\n",
    "    arima_model = ARIMA(train['transaction_id'].dropna(), order=(5,1,0))  # Adjust order based on ACF/PACF analysis\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    arima_forecast = arima_model_fit.forecast(steps=30)\n",
    "\n",
    "    # Evaluate ARIMA model\n",
    "    arima_rmse = np.sqrt(mean_squared_error(test['transaction_id'], arima_forecast))\n",
    "    arima_mae = mean_absolute_error(test['transaction_id'], arima_forecast)\n",
    "\n",
    "    # Display ARIMA Forecasted Data and Evaluation Metrics\n",
    "    print(\"ARIMA Model - 30 Day Forecasted Footfall:\")\n",
    "    print(arima_forecast)\n",
    "    print(f'ARIMA Model - RMSE: {arima_rmse}')\n",
    "    print(f'ARIMA Model - MAE: {arima_mae}')\n",
    "\n",
    "    # Plotting ARIMA Forecast\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(daily_footfall.index, daily_footfall['transaction_id'], label='Observed', color='blue')\n",
    "    plt.plot(pd.date_range(daily_footfall.index[-30], periods=30, freq='D'), arima_forecast, label='Forecast', color='orange')\n",
    "    plt.title('ARIMA Model: Observed vs Forecasted Footfall')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Footfall')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred with ARIMA model: {e}\")\n",
    "\n",
    "# Prophet Model - Alternative Time-Series Forecasting\n",
    "daily_footfall_prophet = daily_footfall.reset_index()\n",
    "daily_footfall_prophet.columns = ['ds', 'y']\n",
    "\n",
    "prophet_model = Prophet()\n",
    "prophet_model.fit(daily_footfall_prophet)\n",
    "future = prophet_model.make_future_dataframe(periods=30)  # Match the length of the test set\n",
    "forecast_prophet = prophet_model.predict(future)\n",
    "\n",
    "# Evaluate Prophet model\n",
    "try:\n",
    "    # Extracting the last 30 days of forecast data for comparison\n",
    "    prophet_forecast_last_30 = forecast_prophet[['ds', 'yhat']].tail(30)\n",
    "    prophet_test = daily_footfall_prophet.tail(30).set_index('ds')\n",
    "\n",
    "    prophet_rmse = np.sqrt(mean_squared_error(prophet_test['y'], prophet_forecast_last_30['yhat']))\n",
    "    prophet_mae = mean_absolute_error(prophet_test['y'], prophet_forecast_last_30['yhat'])\n",
    "\n",
    "    # Display Prophet Forecasted Data and Evaluation Metrics\n",
    "    print(\"Prophet Model - 30 Day Forecasted Footfall:\")\n",
    "    print(prophet_forecast_last_30)\n",
    "    print(f'Prophet Model - RMSE: {prophet_rmse}')\n",
    "    print(f'Prophet Model - MAE: {prophet_mae}')\n",
    "\n",
    "    # Plotting Prophet Forecast\n",
    "    fig = prophet_model.plot(forecast_prophet)\n",
    "    plt.title('Prophet Model Forecast')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Footfall')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred with Prophet model: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6114d9a-830a-4db5-90ea-03dcb4b0c066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d848ebc-b8e3-4502-add1-32719f532edd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a08bf23-e0c2-4401-ac8d-0598acf436da",
   "metadata": {},
   "source": [
    "DAILY FORECASTING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869bc62d-bb3e-428a-bb9a-e097b6adbda3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from pmdarima import auto_arima  # Use auto_arima for automatic ARIMA order selection\n",
    "from prophet import Prophet\n",
    "import numpy as np\n",
    "\n",
    "# Load Dataset\n",
    "df = pd.read_csv('transaction.csv')  # Replace with your dataset path\n",
    "\n",
    "# Data Preprocessing\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Aggregating the Footfall at Different Levels\n",
    "daily_footfall = df.resample('D').count()  # Daily footfall\n",
    "\n",
    "# Exploratory Data Analysis (EDA) - Visualizing Trends\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(daily_footfall, label='Daily Footfall', color='blue')\n",
    "plt.title('Daily Metro Footfall Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Footfall')\n",
    "plt.legend()  # Adding legend\n",
    "plt.show()\n",
    "\n",
    "# Stationarity Check using Augmented Dickey-Fuller Test\n",
    "result = adfuller(daily_footfall['transaction_id'])\n",
    "print(f'ADF Statistic: {result[0]}')\n",
    "print(f'p-value: {result[1]}')\n",
    "\n",
    "# Time-Series Decomposition\n",
    "decomposition = seasonal_decompose(daily_footfall['transaction_id'], model='additive')\n",
    "fig = decomposition.plot()\n",
    "fig.suptitle('Time-Series Decomposition: Trend, Seasonal, and Residuals', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Use the entire dataset for training the ARIMA model\n",
    "try:\n",
    "    # Automatically determine ARIMA order\n",
    "    auto_model = auto_arima(daily_footfall['transaction_id'].dropna(), seasonal=False, stepwise=True)\n",
    "    print(\"Auto ARIMA Model Summary:\")\n",
    "    print(auto_model.summary())\n",
    "\n",
    "    # Fit ARIMA model with the best parameters\n",
    "    arima_model = ARIMA(daily_footfall['transaction_id'], order=auto_model.order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    \n",
    "    # Forecasting the next 30 days\n",
    "    arima_forecast = arima_model_fit.forecast(steps=365)\n",
    "    \n",
    "    # Creating a DataFrame for ARIMA Forecast\n",
    "    arima_forecast_dates = pd.date_range(daily_footfall.index[-1] + pd.Timedelta(days=1), periods=365)\n",
    "    arima_forecast_df = pd.DataFrame({'timestamp': arima_forecast_dates, 'ARIMA_Forecast': arima_forecast})\n",
    "\n",
    "    # Display ARIMA Forecasted Data\n",
    "    print(\"ARIMA Model - 30 Day Forecasted Footfall:\")\n",
    "    print(arima_forecast_df)\n",
    "\n",
    "    # Plotting ARIMA Forecast\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(daily_footfall.index, daily_footfall['transaction_id'], label='Observed', color='blue')\n",
    "    plt.plot(arima_forecast_df['timestamp'], arima_forecast_df['ARIMA_Forecast'], label='Forecast', color='orange')\n",
    "    plt.title('ARIMA Model: Observed vs Forecasted Footfall')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Footfall')\n",
    "    plt.legend()  # Adding legend\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred with ARIMA model: {e}\")\n",
    "\n",
    "# Prophet Model - Alternative Time-Series Forecasting\n",
    "daily_footfall_prophet = daily_footfall.reset_index()\n",
    "\n",
    "# Drop any additional columns and rename correctly\n",
    "daily_footfall_prophet.columns = ['ds', 'y']  # Rename for Prophet\n",
    "\n",
    "# Ensure 'ds' is datetime and 'y' is numeric\n",
    "daily_footfall_prophet['ds'] = pd.to_datetime(daily_footfall_prophet['ds'])\n",
    "daily_footfall_prophet['y'] = pd.to_numeric(daily_footfall_prophet['y'], errors='coerce')\n",
    "\n",
    "# Fit Prophet model\n",
    "prophet_model = Prophet()\n",
    "prophet_model.fit(daily_footfall_prophet)\n",
    "future = prophet_model.make_future_dataframe(periods=365)  # Forecast 30 days into the future\n",
    "forecast_prophet = prophet_model.predict(future)\n",
    "\n",
    "# Creating a DataFrame for Prophet Forecast\n",
    "prophet_forecast_df = forecast_prophet[['ds', 'yhat']].tail(365)\n",
    "prophet_forecast_df.columns = ['timestamp', 'Prophet_Forecast']\n",
    "\n",
    "# Merging ARIMA and Prophet Forecasts\n",
    "forecast_df = pd.merge(arima_forecast_df, prophet_forecast_df, on='timestamp', how='outer')\n",
    "\n",
    "# Save to CSV file\n",
    "forecast_df.to_csv(r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\daily_forecasted_data.csv', index=False)\n",
    "\n",
    "# Display the merged forecast DataFrame\n",
    "print(forecast_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f35eaf-9e99-4b9d-81a2-c9a0ea71d36c",
   "metadata": {},
   "source": [
    "forecasting daily , weekly , monthly , yearly , weekend and weekday txns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1966b22c-4e51-4a6e-900a-ad97a1d15c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from pmdarima import auto_arima  # Use auto_arima for automatic ARIMA order selection\n",
    "from prophet import Prophet\n",
    "import numpy as np\n",
    "\n",
    "# Load Dataset\n",
    "df = pd.read_csv('transaction.csv')  # Replace with your dataset path\n",
    "\n",
    "# Data Preprocessing\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Aggregating the Footfall at Different Levels\n",
    "daily_footfall = df.resample('D').count()  # Daily footfall\n",
    "\n",
    "# Calculate Weekly, Monthly, Yearly, Weekend, and Weekday Footfall\n",
    "weekly_footfall = df.resample('W').count()  # Weekly footfall\n",
    "monthly_footfall = df.resample('M').count()  # Monthly footfall\n",
    "yearly_footfall = df.resample('Y').count()  # Yearly footfall\n",
    "\n",
    "# Weekend and Weekday Footfall\n",
    "# Create a 'day_of_week' column for easier filtering\n",
    "df['day_of_week'] = df.index.dayofweek  # Monday=0, Sunday=6\n",
    "weekend_footfall = df[df['day_of_week'] >= 5].resample('D').count()  # Weekend (Saturday=5, Sunday=6)\n",
    "weekday_footfall = df[df['day_of_week'] < 5].resample('D').count()  # Weekday (Monday=0 to Friday=4)\n",
    "\n",
    "# Drop the 'day_of_week' column from aggregated data\n",
    "df.drop(columns=['day_of_week'], inplace=True)\n",
    "\n",
    "# Exploratory Data Analysis (EDA) - Visualizing Trends\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(daily_footfall, label='Daily Footfall', color='blue')\n",
    "plt.title('Daily Metro Footfall Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Footfall')\n",
    "plt.legend()  # Adding legend\n",
    "plt.show()\n",
    "\n",
    "# Stationarity Check using Augmented Dickey-Fuller Test\n",
    "result = adfuller(daily_footfall['transaction_id'])\n",
    "print(f'ADF Statistic: {result[0]}')\n",
    "print(f'p-value: {result[1]}')\n",
    "\n",
    "# Time-Series Decomposition\n",
    "decomposition = seasonal_decompose(daily_footfall['transaction_id'], model='additive')\n",
    "fig = decomposition.plot()\n",
    "fig.suptitle('Time-Series Decomposition: Trend, Seasonal, and Residuals', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Use the entire dataset for training the ARIMA model\n",
    "try:\n",
    "    # Automatically determine ARIMA order\n",
    "    auto_model = auto_arima(daily_footfall['transaction_id'].dropna(), seasonal=False, stepwise=True)\n",
    "    print(\"Auto ARIMA Model Summary:\")\n",
    "    print(auto_model.summary())\n",
    "\n",
    "    # Fit ARIMA model with the best parameters\n",
    "    arima_model = ARIMA(daily_footfall['transaction_id'], order=auto_model.order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    \n",
    "    # Forecasting the next 365 days\n",
    "    arima_forecast = arima_model_fit.forecast(steps=365)\n",
    "    \n",
    "    # Creating a DataFrame for ARIMA Forecast\n",
    "    arima_forecast_dates = pd.date_range(daily_footfall.index[-1] + pd.Timedelta(days=1), periods=365)\n",
    "    arima_forecast_df = pd.DataFrame({'timestamp': arima_forecast_dates, 'ARIMA_Forecast': arima_forecast})\n",
    "\n",
    "    # Display ARIMA Forecasted Data\n",
    "    print(\"ARIMA Model - 365 Day Forecasted Footfall:\")\n",
    "    print(arima_forecast_df)\n",
    "\n",
    "    # Plotting ARIMA Forecast\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(daily_footfall.index, daily_footfall['transaction_id'], label='Observed', color='blue')\n",
    "    plt.plot(arima_forecast_df['timestamp'], arima_forecast_df['ARIMA_Forecast'], label='Forecast', color='orange')\n",
    "    plt.title('ARIMA Model: Observed vs Forecasted Footfall')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Footfall')\n",
    "    plt.legend()  # Adding legend\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred with ARIMA model: {e}\")\n",
    "\n",
    "# Prophet Model - Alternative Time-Series Forecasting\n",
    "daily_footfall_prophet = daily_footfall.reset_index()\n",
    "\n",
    "# Drop any additional columns and rename correctly\n",
    "daily_footfall_prophet.columns = ['ds', 'y']  # Rename for Prophet\n",
    "\n",
    "# Ensure 'ds' is datetime and 'y' is numeric\n",
    "daily_footfall_prophet['ds'] = pd.to_datetime(daily_footfall_prophet['ds'])\n",
    "daily_footfall_prophet['y'] = pd.to_numeric(daily_footfall_prophet['y'], errors='coerce')\n",
    "\n",
    "# Fit Prophet model\n",
    "prophet_model = Prophet()\n",
    "prophet_model.fit(daily_footfall_prophet)\n",
    "future = prophet_model.make_future_dataframe(periods=365)  # Forecast 365 days into the future\n",
    "forecast_prophet = prophet_model.predict(future)\n",
    "\n",
    "# Creating a DataFrame for Prophet Forecast\n",
    "prophet_forecast_df = forecast_prophet[['ds', 'yhat']].tail(365)\n",
    "prophet_forecast_df.columns = ['timestamp', 'transaction_id']\n",
    "\n",
    "# Save Prophet Forecast to CSV\n",
    "prophet_forecast_df.to_csv(r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors\\daily_footfall.csv', index=False)\n",
    "\n",
    "# Display the Prophet Forecast DataFrame\n",
    "print(prophet_forecast_df.head())\n",
    "\n",
    "# The following lines are commented out or removed to focus on saving only the Prophet forecast\n",
    "# Save Aggregated Data to CSV\n",
    "weekly_footfall.to_csv(r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors\\weekly_footfall.csv')\n",
    "monthly_footfall.to_csv(r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors\\monthly_footfall.csv')\n",
    "yearly_footfall.to_csv(r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors\\yearly_footfall.csv')\n",
    "weekend_footfall.to_csv(r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors\\weekend_footfall.csv')\n",
    "weekday_footfall.to_csv(r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors\\weekday_footfall.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a97ab7-c6ef-433c-bfcc-c3cba2c91a19",
   "metadata": {},
   "source": [
    "GRAPH PLOTTING FOR EVERY ASPECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4065abea-e018-462b-935c-39bb7a661e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_raw_data(data_type, file_paths):\n",
    "    \"\"\"\n",
    "    Plots raw data based on the specified data type from the given file paths.\n",
    "    Additionally plots weekday vs. weekend data if 'weekday_vs_weekend' is specified.\n",
    "\n",
    "    Parameters:\n",
    "    - data_type (str): Type of data to plot ('daily', 'weekly', 'monthly', 'yearly', 'weekend', 'weekday', 'weekday_vs_weekend').\n",
    "    - file_paths (dict): Dictionary containing file paths for each data type.\n",
    "    \"\"\"\n",
    "    if data_type == 'weekday_vs_weekend':\n",
    "        # Load Weekend Dataset\n",
    "        weekend_file_path = file_paths['weekend']\n",
    "        weekend_df = pd.read_csv(weekend_file_path)\n",
    "        weekend_df['timestamp'] = pd.to_datetime(weekend_df['timestamp'])\n",
    "        weekend_df.set_index('timestamp', inplace=True)\n",
    "        \n",
    "        # Load Weekday Dataset\n",
    "        weekday_file_path = file_paths['weekday']\n",
    "        weekday_df = pd.read_csv(weekday_file_path)\n",
    "        weekday_df['timestamp'] = pd.to_datetime(weekday_df['timestamp'])\n",
    "        weekday_df.set_index('timestamp', inplace=True)\n",
    "        \n",
    "        # Print the raw data to check its structure\n",
    "        print(\"Raw weekend data:\")\n",
    "        print(weekend_df.head())\n",
    "        print(\"Raw weekday data:\")\n",
    "        print(weekday_df.head())\n",
    "        \n",
    "        # Plotting Weekday vs Weekend\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(weekend_df.index, weekend_df['transaction_id'], label='Weekend Footfall', color='purple')\n",
    "        plt.plot(weekday_df.index, weekday_df['transaction_id'], label='Weekday Footfall', color='cyan')\n",
    "        plt.title('Weekday vs Weekend Footfall Over Time')\n",
    "        plt.xlabel('Timestamp')\n",
    "        plt.ylabel('Transaction ID')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    \n",
    "    elif data_type in file_paths:\n",
    "        # Load Dataset\n",
    "        file_path = file_paths[data_type]\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Data Preprocessing\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        df.set_index('timestamp', inplace=True)\n",
    "        \n",
    "        # Print the raw data to check its structure\n",
    "        print(f\"Raw data for {data_type}:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        # Plotting the raw data\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(df.index, df['transaction_id'], label=f'Raw {data_type.capitalize()} Footfall', color='blue')\n",
    "        plt.title(f'Raw {data_type.capitalize()} Footfall Over Time')\n",
    "        plt.xlabel('Timestamp')\n",
    "        plt.ylabel('Transaction ID')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    \n",
    "    else:\n",
    "        print(f\"File path for '{data_type}' data type is not provided.\")\n",
    "\n",
    "# Example usage\n",
    "file_paths = {\n",
    "    'daily': r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors\\daily_footfall.csv',\n",
    "    'weekly': r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors\\weekly_footfall.csv',\n",
    "    'monthly': r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors\\monthly_footfall.csv',\n",
    "    'yearly': r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors\\yearly_footfall.csv',\n",
    "    'weekend': r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors\\weekend_footfall.csv',\n",
    "    'weekday': r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors\\weekday_footfall.csv'\n",
    "}\n",
    "\n",
    "data_type = 'weekly'  # Replace with desired data type ('daily', 'weekly', 'monthly', 'yearly', 'weekend', 'weekday', 'weekday_vs_weekend')\n",
    "plot_raw_data(data_type, file_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d813ea0-a991-4244-b661-96619cb3fb67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8983837b-abc1-44d5-9b99-39299e7a75e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_raw_data(data_type, file_paths):\n",
    "    \"\"\"\n",
    "    Plots raw data based on the specified data type from the given file paths.\n",
    "    Additionally plots weekday vs. weekend data if 'weekday_vs_weekend' is specified.\n",
    "    Aggregates daily data into monthly totals if 'monthly_aggregation' is specified.\n",
    "\n",
    "    Parameters:\n",
    "    - data_type (str): Type of data to plot ('daily', 'weekly', 'monthly', 'yearly', 'weekend', 'weekday', 'weekday_vs_weekend', 'monthly_aggregation').\n",
    "    - file_paths (dict): Dictionary containing file paths for each data type.\n",
    "    \"\"\"\n",
    "    if data_type == 'weekday_vs_weekend':\n",
    "        # Load Weekend Dataset\n",
    "        weekend_file_path = file_paths['weekend']\n",
    "        weekend_df = pd.read_csv(weekend_file_path)\n",
    "        weekend_df['timestamp'] = pd.to_datetime(weekend_df['timestamp'])\n",
    "        weekend_df.set_index('timestamp', inplace=True)\n",
    "        \n",
    "        # Load Weekday Dataset\n",
    "        weekday_file_path = file_paths['weekday']\n",
    "        weekday_df = pd.read_csv(weekday_file_path)\n",
    "        weekday_df['timestamp'] = pd.to_datetime(weekday_df['timestamp'])\n",
    "        weekday_df.set_index('timestamp', inplace=True)\n",
    "        \n",
    "        # Print the raw data to check its structure\n",
    "        print(\"Raw weekend data:\")\n",
    "        print(weekend_df.head())\n",
    "        print(\"Raw weekday data:\")\n",
    "        print(weekday_df.head())\n",
    "        \n",
    "        # Plotting Weekday vs Weekend\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(weekend_df.index, weekend_df['transaction_id'], label='Weekend Footfall', color='purple')\n",
    "        plt.plot(weekday_df.index, weekday_df['transaction_id'], label='Weekday Footfall', color='cyan')\n",
    "        plt.title('Weekday vs Weekend Footfall Over Time')\n",
    "        plt.xlabel('Timestamp')\n",
    "        plt.ylabel('Transaction ID')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    \n",
    "    elif data_type == 'monthly_aggregation':\n",
    "        # Load Daily Dataset\n",
    "        daily_file_path = file_paths['daily']\n",
    "        daily_df = pd.read_csv(daily_file_path)\n",
    "        daily_df['timestamp'] = pd.to_datetime(daily_df['timestamp'])\n",
    "        daily_df.set_index('timestamp', inplace=True)\n",
    "        \n",
    "        # Aggregate Data to Monthly Totals\n",
    "        monthly_footfall = daily_df.resample('M').sum()  # Aggregating daily data into monthly totals\n",
    "        \n",
    "        # Print the aggregated data\n",
    "        print(\"Monthly Aggregated Data:\")\n",
    "        print(monthly_footfall.head())\n",
    "        \n",
    "        # Optionally, save the aggregated data to a new CSV file\n",
    "        aggregated_file_path = r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors\\monthly_aggregated_data.csv'\n",
    "        monthly_footfall.to_csv(aggregated_file_path)\n",
    "        print(f\"Aggregated data saved to {aggregated_file_path}\")\n",
    "        \n",
    "        # Plotting the aggregated monthly data\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(monthly_footfall.index, monthly_footfall['transaction_id'], label='Monthly Footfall', color='orange')\n",
    "        plt.title('Monthly Footfall Aggregated from Daily Data')\n",
    "        plt.xlabel('Timestamp')\n",
    "        plt.ylabel('Transaction ID')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    \n",
    "    elif data_type in file_paths:\n",
    "        # Load Dataset\n",
    "        file_path = file_paths[data_type]\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Data Preprocessing\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        df.set_index('timestamp', inplace=True)\n",
    "        \n",
    "        # Print the raw data to check its structure\n",
    "        print(f\"Raw data for {data_type}:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        # Plotting the raw data\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(df.index, df['transaction_id'], label=f'Raw {data_type.capitalize()} Footfall', color='blue')\n",
    "        plt.title(f'Raw {data_type.capitalize()} Footfall Over Time')\n",
    "        plt.xlabel('Timestamp')\n",
    "        plt.ylabel('Transaction ID')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    \n",
    "    else:\n",
    "        print(f\"File path for '{data_type}' data type is not provided.\")\n",
    "\n",
    "# Example usage\n",
    "file_paths = {\n",
    "    'daily': r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors\\daily_footfall.csv',\n",
    "    'weekly': r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors\\weekly_footfall.csv',\n",
    "    'monthly': r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors\\monthly_footfall.csv',\n",
    "    'yearly': r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors\\yearly_footfall.csv',\n",
    "    'weekend': r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors\\weekend_footfall.csv',\n",
    "    'weekday': r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors\\weekday_footfall.csv'\n",
    "}\n",
    "\n",
    "data_type = 'weekday_vs_weekend'  # Replace with desired data type ('daily', 'weekly', 'monthly', 'yearly', 'weekend', 'weekday', 'weekday_vs_weekend', 'monthly_aggregation')\n",
    "plot_raw_data(data_type, file_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d06f15-6ecd-4dd8-821d-0907062ce786",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9f55198-aafd-4a10-8e80-770d51b34f2b",
   "metadata": {},
   "source": [
    "new forecast "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91dc817-e12f-4288-af75-f9ba6971ad17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from pmdarima import auto_arima\n",
    "from prophet import Prophet\n",
    "import numpy as np\n",
    "\n",
    "# Load Dataset\n",
    "df = pd.read_csv('transaction.csv')  # Replace with your dataset path\n",
    "\n",
    "# Data Preprocessing\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Aggregating the Footfall at Different Levels\n",
    "daily_footfall = df.resample('D').count()  # Daily footfall\n",
    "\n",
    "# Time-Series Decomposition\n",
    "decomposition = seasonal_decompose(daily_footfall['transaction_id'], model='additive')\n",
    "fig = decomposition.plot()\n",
    "fig.suptitle('Time-Series Decomposition: Trend, Seasonal, and Residuals', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# ARIMA Forecasting\n",
    "try:\n",
    "    # Automatically determine ARIMA order\n",
    "    auto_model = auto_arima(daily_footfall['transaction_id'].dropna(), seasonal=False, stepwise=True)\n",
    "    print(\"Auto ARIMA Model Summary:\")\n",
    "    print(auto_model.summary())\n",
    "\n",
    "    # Fit ARIMA model with the best parameters\n",
    "    arima_model = ARIMA(daily_footfall['transaction_id'], order=auto_model.order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    \n",
    "    # Forecasting the next 365 days\n",
    "    arima_forecast = arima_model_fit.forecast(steps=365)\n",
    "    \n",
    "    # Creating a DataFrame for ARIMA Forecast\n",
    "    arima_forecast_dates = pd.date_range(daily_footfall.index[-1] + pd.Timedelta(days=1), periods=365)\n",
    "    arima_forecast_df = pd.DataFrame({'timestamp': arima_forecast_dates, 'ARIMA_Forecast': arima_forecast})\n",
    "\n",
    "    # Display ARIMA Forecasted Data\n",
    "    print(\"ARIMA Model - 365 Day Forecasted Footfall:\")\n",
    "    print(arima_forecast_df)\n",
    "\n",
    "    # Aggregation of ARIMA Forecast\n",
    "    daily_forecast = arima_forecast_df.copy()\n",
    "    daily_forecast.set_index('timestamp', inplace=True)\n",
    "    weekly_forecast = daily_forecast.resample('W').sum()\n",
    "    monthly_forecast = daily_forecast.resample('M').sum()\n",
    "    yearly_forecast = daily_forecast.resample('Y').sum()\n",
    "    weekend_forecast = daily_forecast[daily_forecast.index.dayofweek >= 5].resample('D').sum()\n",
    "    weekday_forecast = daily_forecast[daily_forecast.index.dayofweek < 5].resample('D').sum()\n",
    "\n",
    "    # Plot ARIMA Forecast\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(daily_footfall.index, daily_footfall['transaction_id'], label='Observed', color='blue')\n",
    "    plt.plot(arima_forecast_df['timestamp'], arima_forecast_df['ARIMA_Forecast'], label='ARIMA Forecast', color='orange')\n",
    "    plt.title('ARIMA Model: Observed vs Forecasted Footfall')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Footfall')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred with ARIMA model: {e}\")\n",
    "\n",
    "# Prophet Model - Alternative Time-Series Forecasting\n",
    "daily_footfall_prophet = daily_footfall.reset_index()\n",
    "daily_footfall_prophet.columns = ['ds', 'y']\n",
    "daily_footfall_prophet['ds'] = pd.to_datetime(daily_footfall_prophet['ds'])\n",
    "daily_footfall_prophet['y'] = pd.to_numeric(daily_footfall_prophet['y'], errors='coerce')\n",
    "\n",
    "# Fit Prophet model\n",
    "prophet_model = Prophet()\n",
    "prophet_model.fit(daily_footfall_prophet)\n",
    "future = prophet_model.make_future_dataframe(periods=365)  # Forecast 365 days into the future\n",
    "forecast_prophet = prophet_model.predict(future)\n",
    "\n",
    "# Creating a DataFrame for Prophet Forecast\n",
    "prophet_forecast_df = forecast_prophet[['ds', 'yhat']].tail(365)\n",
    "prophet_forecast_df.columns = ['timestamp', 'transaction_id']\n",
    "\n",
    "# Save Prophet Forecast to CSV\n",
    "prophet_forecast_df.to_csv(r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors\\daily_footfall_forecast_prophet.csv', index=False)\n",
    "\n",
    "# Aggregation of Prophet Forecast\n",
    "daily_forecast_prophet = prophet_forecast_df.copy()\n",
    "daily_forecast_prophet.set_index('timestamp', inplace=True)\n",
    "weekly_forecast_prophet = daily_forecast_prophet.resample('W').sum()\n",
    "monthly_forecast_prophet = daily_forecast_prophet.resample('M').sum()\n",
    "yearly_forecast_prophet = daily_forecast_prophet.resample('Y').sum()\n",
    "weekend_forecast_prophet = daily_forecast_prophet[daily_forecast_prophet.index.dayofweek >= 5].resample('D').sum()\n",
    "weekday_forecast_prophet = daily_forecast_prophet[daily_forecast_prophet.index.dayofweek < 5].resample('D').sum()\n",
    "\n",
    "# Save Aggregated Forecast Data to CSV\n",
    "weekly_forecast_prophet.to_csv(r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors\\weekly_forecast_prophet.csv')\n",
    "monthly_forecast_prophet.to_csv(r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors\\monthly_forecast_prophet.csv')\n",
    "yearly_forecast_prophet.to_csv(r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors\\yearly_forecast_prophet.csv')\n",
    "weekend_forecast_prophet.to_csv(r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors\\weekend_forecast_prophet.csv')\n",
    "weekday_forecast_prophet.to_csv(r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors\\weekday_forecast_prophet.csv')\n",
    "\n",
    "# Display the Prophet Forecast DataFrame\n",
    "print(prophet_forecast_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a371ef04-9583-4f05-982b-7ae1bd6de3d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb533de1-c727-4243-a967-1fac1e8c4cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from pmdarima import auto_arima\n",
    "from prophet import Prophet\n",
    "import numpy as np\n",
    "\n",
    "# Load Dataset\n",
    "df = pd.read_csv('transaction.csv')  # Replace with your dataset path\n",
    "\n",
    "# Data Preprocessing\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Aggregating the Footfall at Different Levels\n",
    "daily_footfall = df.resample('D').count()  # Daily footfall\n",
    "\n",
    "# Time-Series Decomposition\n",
    "decomposition = seasonal_decompose(daily_footfall['transaction_id'], model='additive')\n",
    "fig = decomposition.plot()\n",
    "fig.suptitle('Time-Series Decomposition: Trend, Seasonal, and Residuals', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# ARIMA Forecasting\n",
    "try:\n",
    "    # Automatically determine ARIMA order\n",
    "    auto_model = auto_arima(daily_footfall['transaction_id'].dropna(), seasonal=False, stepwise=True)\n",
    "    print(\"Auto ARIMA Model Summary:\")\n",
    "    print(auto_model.summary())\n",
    "\n",
    "    # Fit ARIMA model with the best parameters\n",
    "    arima_model = ARIMA(daily_footfall['transaction_id'], order=auto_model.order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    \n",
    "    # Forecasting the next 365 days\n",
    "    arima_forecast = arima_model_fit.forecast(steps=365)\n",
    "    \n",
    "    # Creating a DataFrame for ARIMA Forecast\n",
    "    arima_forecast_dates = pd.date_range(daily_footfall.index[-1] + pd.Timedelta(days=1), periods=365)\n",
    "    arima_forecast_df = pd.DataFrame({'timestamp': arima_forecast_dates, 'ARIMA_Forecast': arima_forecast})\n",
    "\n",
    "    # Display ARIMA Forecasted Data\n",
    "    print(\"ARIMA Model - 365 Day Forecasted Footfall:\")\n",
    "    print(arima_forecast_df)\n",
    "\n",
    "    # Aggregation of ARIMA Forecast\n",
    "    daily_forecast = arima_forecast_df.copy()\n",
    "    daily_forecast.set_index('timestamp', inplace=True)\n",
    "    weekly_forecast = daily_forecast.resample('W').sum()\n",
    "    monthly_forecast = daily_forecast.resample('M').sum()\n",
    "    yearly_forecast = daily_forecast.resample('Y').sum()\n",
    "    weekend_forecast = daily_forecast[daily_forecast.index.dayofweek >= 5].resample('D').sum()\n",
    "    weekday_forecast = daily_forecast[daily_forecast.index.dayofweek < 5].resample('D').sum()\n",
    "\n",
    "    # Add week of year to weekly forecast\n",
    "    weekly_forecast['week_of_year'] = weekly_forecast.index.isocalendar().week\n",
    "\n",
    "    # Add month name to monthly forecast\n",
    "    monthly_forecast['month_name'] = monthly_forecast.index.month_name()\n",
    "\n",
    "    # Plot ARIMA Forecast\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(daily_footfall.index, daily_footfall['transaction_id'], label='Observed', color='blue')\n",
    "    plt.plot(arima_forecast_df['timestamp'], arima_forecast_df['ARIMA_Forecast'], label='ARIMA Forecast', color='orange')\n",
    "    plt.title('ARIMA Model: Observed vs Forecasted Footfall')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Footfall')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred with ARIMA model: {e}\")\n",
    "\n",
    "# Prophet Model - Alternative Time-Series Forecasting\n",
    "daily_footfall_prophet = daily_footfall.reset_index()\n",
    "daily_footfall_prophet.columns = ['ds', 'y']\n",
    "daily_footfall_prophet['ds'] = pd.to_datetime(daily_footfall_prophet['ds'])\n",
    "daily_footfall_prophet['y'] = pd.to_numeric(daily_footfall_prophet['y'], errors='coerce')\n",
    "\n",
    "# Fit Prophet model\n",
    "prophet_model = Prophet()\n",
    "prophet_model.fit(daily_footfall_prophet)\n",
    "future = prophet_model.make_future_dataframe(periods=365)  # Forecast 365 days into the future\n",
    "forecast_prophet = prophet_model.predict(future)\n",
    "\n",
    "# Creating a DataFrame for Prophet Forecast\n",
    "prophet_forecast_df = forecast_prophet[['ds', 'yhat']].tail(365)\n",
    "prophet_forecast_df.columns = ['timestamp', 'transaction_id']\n",
    "\n",
    "# Save Prophet Forecast to CSV\n",
    "prophet_forecast_df.to_csv(r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors\\daily_footfall_forecast_prophet.csv', index=False)\n",
    "\n",
    "# Aggregation of Prophet Forecast\n",
    "daily_forecast_prophet = prophet_forecast_df.copy()\n",
    "daily_forecast_prophet.set_index('timestamp', inplace=True)\n",
    "weekly_forecast_prophet = daily_forecast_prophet.resample('W').sum()\n",
    "monthly_forecast_prophet = daily_forecast_prophet.resample('M').sum()\n",
    "yearly_forecast_prophet = daily_forecast_prophet.resample('Y').sum()\n",
    "weekend_forecast_prophet = daily_forecast_prophet[daily_forecast_prophet.index.dayofweek >= 5].resample('D').sum()\n",
    "weekday_forecast_prophet = daily_forecast_prophet[daily_forecast_prophet.index.dayofweek < 5].resample('D').sum()\n",
    "\n",
    "# Add week of year to weekly forecast\n",
    "weekly_forecast_prophet['week_of_year'] = weekly_forecast_prophet.index.isocalendar().week\n",
    "\n",
    "# Add month name to monthly forecast\n",
    "monthly_forecast_prophet['month_name'] = monthly_forecast_prophet.index.month_name()\n",
    "\n",
    "# Save Aggregated Forecast Data to CSV\n",
    "weekly_forecast_prophet.to_csv(r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors\\weekly_forecast_prophet.csv')\n",
    "monthly_forecast_prophet.to_csv(r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors\\monthly_forecast_prophet.csv')\n",
    "yearly_forecast_prophet.to_csv(r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors\\yearly_forecast_prophet.csv')\n",
    "weekend_forecast_prophet.to_csv(r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors\\weekend_forecast_prophet.csv')\n",
    "weekday_forecast_prophet.to_csv(r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors\\weekday_forecast_prophet.csv')\n",
    "\n",
    "# Display the Prophet Forecast DataFrame\n",
    "print(prophet_forecast_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82901ef-24bf-47f2-8102-b48051f78790",
   "metadata": {},
   "source": [
    "user input forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677490a6-f7ea-4111-9cf2-b3446a3db4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from pmdarima import auto_arima\n",
    "from prophet import Prophet\n",
    "import numpy as np\n",
    "\n",
    "# Load Dataset\n",
    "df = pd.read_csv('transaction.csv')  # Replace with your dataset path\n",
    "\n",
    "# Data Preprocessing\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Aggregating the Footfall at Different Levels\n",
    "daily_footfall = df.resample('D').count()  # Daily footfall\n",
    "\n",
    "# Input start and end date for forecasting\n",
    "start_date = input(\"Enter the start date for forecasting (YYYY-MM-DD): \")\n",
    "end_date = input(\"Enter the end date for forecasting (YYYY-MM-DD): \")\n",
    "\n",
    "# Convert input dates to pandas datetime\n",
    "start_date = pd.to_datetime(start_date)\n",
    "end_date = pd.to_datetime(end_date)\n",
    "\n",
    "# Number of days for forecasting based on the entered dates\n",
    "forecast_period = (end_date - daily_footfall.index[-1]).days\n",
    "\n",
    "if forecast_period <= 0:\n",
    "    print(\"Error: The end date must be after the last available date in the dataset.\")\n",
    "else:\n",
    "    # ARIMA Forecasting\n",
    "    try:\n",
    "        # Automatically determine ARIMA order\n",
    "        auto_model = auto_arima(daily_footfall['transaction_id'].dropna(), seasonal=False, stepwise=True)\n",
    "        print(\"Auto ARIMA Model Summary:\")\n",
    "        print(auto_model.summary())\n",
    "\n",
    "        # Fit ARIMA model with the best parameters\n",
    "        arima_model = ARIMA(daily_footfall['transaction_id'], order=auto_model.order)\n",
    "        arima_model_fit = arima_model.fit()\n",
    "\n",
    "        # Forecast for the specified period\n",
    "        arima_forecast = arima_model_fit.forecast(steps=forecast_period)\n",
    "\n",
    "        # Creating a DataFrame for ARIMA Forecast\n",
    "        arima_forecast_dates = pd.date_range(daily_footfall.index[-1] + pd.Timedelta(days=1), periods=forecast_period)\n",
    "        arima_forecast_df = pd.DataFrame({'timestamp': arima_forecast_dates, 'ARIMA_Forecast': arima_forecast})\n",
    "\n",
    "        # Display ARIMA Forecasted Data\n",
    "        print(\"ARIMA Model - Forecasted Footfall:\")\n",
    "        print(arima_forecast_df)\n",
    "\n",
    "        # Aggregation of ARIMA Forecast\n",
    "        daily_forecast = arima_forecast_df.copy()\n",
    "        daily_forecast.set_index('timestamp', inplace=True)\n",
    "        weekly_forecast = daily_forecast.resample('W').sum()\n",
    "        monthly_forecast = daily_forecast.resample('M').sum()\n",
    "        yearly_forecast = daily_forecast.resample('Y').sum()\n",
    "        weekend_forecast = daily_forecast[daily_forecast.index.dayofweek >= 5].resample('D').sum()\n",
    "        weekday_forecast = daily_forecast[daily_forecast.index.dayofweek < 5].resample('D').sum()\n",
    "\n",
    "        # Plot ARIMA Forecast\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(daily_footfall.index, daily_footfall['transaction_id'], label='Observed', color='blue')\n",
    "        plt.plot(arima_forecast_df['timestamp'], arima_forecast_df['ARIMA_Forecast'], label='ARIMA Forecast', color='orange')\n",
    "        plt.title(f'ARIMA Model: Observed vs Forecasted Footfall ({start_date} to {end_date})')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Footfall')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred with ARIMA model: {e}\")\n",
    "\n",
    "    # Prophet Model - Alternative Time-Series Forecasting\n",
    "    daily_footfall_prophet = daily_footfall.reset_index()\n",
    "    daily_footfall_prophet.columns = ['ds', 'y']\n",
    "    daily_footfall_prophet['ds'] = pd.to_datetime(daily_footfall_prophet['ds'])\n",
    "    daily_footfall_prophet['y'] = pd.to_numeric(daily_footfall_prophet['y'], errors='coerce')\n",
    "\n",
    "    # Fit Prophet model\n",
    "    prophet_model = Prophet()\n",
    "    prophet_model.fit(daily_footfall_prophet)\n",
    "\n",
    "    # Generate future dataframe for the specified forecasting period\n",
    "    future_dates = pd.date_range(daily_footfall.index[-1] + pd.Timedelta(days=1), end=end_date, freq='D')\n",
    "    future_df = pd.DataFrame({'ds': future_dates})\n",
    "\n",
    "    # Predict using Prophet model\n",
    "    forecast_prophet = prophet_model.predict(future_df)\n",
    "\n",
    "    # Creating a DataFrame for Prophet Forecast\n",
    "    prophet_forecast_df = forecast_prophet[['ds', 'yhat']]\n",
    "    prophet_forecast_df.columns = ['timestamp', 'transaction_id']\n",
    "\n",
    "    # Display the Prophet Forecast DataFrame\n",
    "    print(prophet_forecast_df.head())\n",
    "\n",
    "    # Save Prophet Forecast to CSV\n",
    "    prophet_forecast_df.to_csv(r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors\\daily_footfall_forecast_prophet.csv', index=False)\n",
    "\n",
    "    # Plot Prophet Forecast\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(daily_footfall.index, daily_footfall['transaction_id'], label='Observed', color='blue')\n",
    "    plt.plot(prophet_forecast_df['timestamp'], prophet_forecast_df['transaction_id'], label='Prophet Forecast', color='green')\n",
    "    plt.title(f'Prophet Model: Observed vs Forecasted Footfall ({start_date} to {end_date})')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Footfall')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Forecasting completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd8d750-e59b-415d-a58a-b6136e49a473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f175302-8d28-403f-bae1-d780efcbb2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from pmdarima import auto_arima\n",
    "from prophet import Prophet\n",
    "import numpy as np\n",
    "\n",
    "# Load Dataset\n",
    "df = pd.read_csv('transaction.csv')  # Replace with your dataset path\n",
    "\n",
    "# Data Preprocessing\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Aggregating the Footfall at Different Levels\n",
    "daily_footfall = df.resample('D').count()  # Daily footfall\n",
    "\n",
    "# Time-Series Decomposition\n",
    "decomposition = seasonal_decompose(daily_footfall['transaction_id'], model='additive')\n",
    "fig = decomposition.plot()\n",
    "fig.suptitle('Time-Series Decomposition: Trend, Seasonal, and Residuals', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Specify Forecast Period\n",
    "start_date = pd.to_datetime(input(\"Enter forecast start date (YYYY-MM-DD): \"))\n",
    "end_date = pd.to_datetime(input(\"Enter forecast end date (YYYY-MM-DD): \"))\n",
    "forecast_period = (end_date - start_date).days\n",
    "\n",
    "# ARIMA Forecasting\n",
    "try:\n",
    "    # Automatically determine ARIMA order\n",
    "    auto_model = auto_arima(daily_footfall['transaction_id'].dropna(), seasonal=False, stepwise=True)\n",
    "    print(\"Auto ARIMA Model Summary:\")\n",
    "    print(auto_model.summary())\n",
    "\n",
    "    # Fit ARIMA model with the best parameters\n",
    "    arima_model = ARIMA(daily_footfall['transaction_id'], order=auto_model.order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "\n",
    "    # Adjust the forecast range based on the start and end dates provided\n",
    "    forecast_start_index = (start_date - daily_footfall.index[-1]).days  # Days from last data point to start date\n",
    "    arima_forecast = arima_model_fit.forecast(steps=(forecast_period + forecast_start_index))\n",
    "\n",
    "    # Creating a DataFrame for ARIMA Forecast\n",
    "    arima_forecast_dates = pd.date_range(daily_footfall.index[-1] + pd.Timedelta(days=1), periods=len(arima_forecast))\n",
    "    arima_forecast_df = pd.DataFrame({'timestamp': arima_forecast_dates, 'ARIMA_Forecast': arima_forecast})\n",
    "\n",
    "    # Filter the forecast to start exactly from the user-specified start date\n",
    "    arima_forecast_df = arima_forecast_df[arima_forecast_df['timestamp'] >= start_date]\n",
    "\n",
    "    # Display ARIMA Forecasted Data\n",
    "    print(f\"ARIMA Model - Forecasted Footfall from {start_date.date()} to {end_date.date()}:\")\n",
    "    print(arima_forecast_df)\n",
    "\n",
    "    # Aggregation of ARIMA Forecast\n",
    "    daily_forecast = arima_forecast_df.copy()\n",
    "    daily_forecast.set_index('timestamp', inplace=True)\n",
    "    weekly_forecast = daily_forecast.resample('W').sum()\n",
    "    monthly_forecast = daily_forecast.resample('M').sum()\n",
    "    yearly_forecast = daily_forecast.resample('Y').sum()\n",
    "    weekend_forecast = daily_forecast[daily_forecast.index.dayofweek >= 5].resample('D').sum()\n",
    "    weekday_forecast = daily_forecast[daily_forecast.index.dayofweek < 5].resample('D').sum()\n",
    "\n",
    "    # Add week of year and day of week to weekly forecast\n",
    "    weekly_forecast['week_of_year'] = weekly_forecast.index.isocalendar().week\n",
    "    weekly_forecast['day_of_week'] = weekly_forecast.index.day_name()\n",
    "\n",
    "    # Add month name to monthly forecast\n",
    "    monthly_forecast['month_name'] = monthly_forecast.index.month_name()\n",
    "\n",
    "    # Add day of week to weekend and weekday forecasts\n",
    "    weekend_forecast['day_of_week'] = weekend_forecast.index.day_name()\n",
    "    weekday_forecast['day_of_week'] = weekday_forecast.index.day_name()\n",
    "\n",
    "    # Plot ARIMA Forecast\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(daily_footfall.index, daily_footfall['transaction_id'], label='Observed', color='blue')\n",
    "    plt.plot(arima_forecast_df['timestamp'], arima_forecast_df['ARIMA_Forecast'], label='ARIMA Forecast', color='orange')\n",
    "    plt.title(f'ARIMA Model: Observed vs Forecasted Footfall ({start_date.date()} to {end_date.date()})')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Footfall')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred with ARIMA model: {e}\")\n",
    "\n",
    "# Prophet Model - Alternative Time-Series Forecasting\n",
    "daily_footfall_prophet = daily_footfall.reset_index()\n",
    "daily_footfall_prophet.columns = ['ds', 'y']\n",
    "daily_footfall_prophet['ds'] = pd.to_datetime(daily_footfall_prophet['ds'])\n",
    "daily_footfall_prophet['y'] = pd.to_numeric(daily_footfall_prophet['y'], errors='coerce')\n",
    "\n",
    "# Fit Prophet model\n",
    "prophet_model = Prophet()\n",
    "prophet_model.fit(daily_footfall_prophet)\n",
    "\n",
    "# Create future dataframe starting from the user-specified start_date\n",
    "future_df = pd.date_range(start=start_date, end=end_date)\n",
    "future_prophet = pd.DataFrame({'ds': future_df})\n",
    "forecast_prophet = prophet_model.predict(future_prophet)\n",
    "\n",
    "# Creating a DataFrame for Prophet Forecast\n",
    "prophet_forecast_df = forecast_prophet[['ds', 'yhat']]\n",
    "prophet_forecast_df.columns = ['timestamp', 'transaction_id']\n",
    "\n",
    "# Save Prophet Forecast to CSV\n",
    "prophet_forecast_df.to_csv(r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors\\daily_footfall_forecast_prophet.csv', index=False)\n",
    "\n",
    "# Aggregation of Prophet Forecast\n",
    "daily_forecast_prophet = prophet_forecast_df.copy()\n",
    "daily_forecast_prophet.set_index('timestamp', inplace=True)\n",
    "weekly_forecast_prophet = daily_forecast_prophet.resample('W').sum()\n",
    "monthly_forecast_prophet = daily_forecast_prophet.resample('M').sum()\n",
    "yearly_forecast_prophet = daily_forecast_prophet.resample('Y').sum()\n",
    "weekend_forecast_prophet = daily_forecast_prophet[daily_forecast_prophet.index.dayofweek >= 5].resample('D').sum()\n",
    "weekday_forecast_prophet = daily_forecast_prophet[daily_forecast_prophet.index.dayofweek < 5].resample('D').sum()\n",
    "\n",
    "# Add week of year and day of week to weekly forecast\n",
    "weekly_forecast_prophet['week_of_year'] = weekly_forecast_prophet.index.isocalendar().week\n",
    "weekly_forecast_prophet['day_of_week'] = weekly_forecast_prophet.index.day_name()\n",
    "\n",
    "# Add month name to monthly forecast\n",
    "monthly_forecast_prophet['month_name'] = monthly_forecast_prophet.index.month_name()\n",
    "\n",
    "# Add day of week to weekend and weekday forecasts\n",
    "weekend_forecast_prophet['day_of_week'] = weekend_forecast_prophet.index.day_name()\n",
    "weekday_forecast_prophet['day_of_week'] = weekday_forecast_prophet.index.day_name()\n",
    "\n",
    "# Save Aggregated Forecast Data to CSV\n",
    "weekly_forecast_prophet.to_csv(r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors\\weekly_forecast_prophet.csv')\n",
    "monthly_forecast_prophet.to_csv(r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors\\monthly_forecast_prophet.csv')\n",
    "yearly_forecast_prophet.to_csv(r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors\\yearly_forecast_prophet.csv')\n",
    "weekend_forecast_prophet.to_csv(r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors\\weekend_forecast_prophet.csv')\n",
    "weekday_forecast_prophet.to_csv(r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors\\weekday_forecast_prophet.csv')\n",
    "\n",
    "# Display the Prophet Forecast DataFrame\n",
    "print(prophet_forecast_df.head())\n",
    "\n",
    "print(\"Files saved to their respective destinations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5421fb14-d7cb-4787-beeb-6164df3d30e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from pmdarima import auto_arima\n",
    "from prophet import Prophet\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load Dataset\n",
    "df = pd.read_csv('transaction_updated.csv')  # Replace with your dataset path\n",
    "\n",
    "# Data Preprocessing\n",
    "# Handling the date format explicitly\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], format='%Y-%m-%dT%H-%M-%S.%fZ', errors='coerce')\n",
    "\n",
    "# Check for any rows where parsing failed\n",
    "invalid_dates = df[df['timestamp'].isnull()]\n",
    "if not invalid_dates.empty:\n",
    "    print(\"Some dates could not be parsed:\")\n",
    "    print(invalid_dates)\n",
    "\n",
    "# Drop rows with invalid dates if needed\n",
    "df = df.dropna(subset=['timestamp'])\n",
    "\n",
    "# Continue with setting the index\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Filtering based on user input for stn no and EqN\n",
    "stn_no = input(\"Enter station number (or 'all' for all stations): \")\n",
    "eqn_no = input(\"Enter equipment number (or 'all' for all equipment): \")\n",
    "\n",
    "# Apply filters\n",
    "if stn_no.lower() != 'all':\n",
    "    df = df[df['stn_no'] == int(stn_no)]\n",
    "if eqn_no.lower() != 'all':\n",
    "    df = df[df['EqN'] == int(eqn_no)]\n",
    "\n",
    "# Aggregating the Footfall at Different Levels\n",
    "daily_footfall = df.resample('D').count()  # Daily footfall\n",
    "\n",
    "# Time-Series Decomposition\n",
    "decomposition = seasonal_decompose(daily_footfall['transaction_id'], model='additive')\n",
    "fig = decomposition.plot()\n",
    "fig.suptitle('Time-Series Decomposition: Trend, Seasonal, and Residuals', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Specify Forecast Period\n",
    "start_date = pd.to_datetime(input(\"Enter forecast start date (YYYY-MM-DD): \"))\n",
    "end_date = pd.to_datetime(input(\"Enter forecast end date (YYYY-MM-DD): \"))\n",
    "forecast_period = (end_date - start_date).days\n",
    "\n",
    "# File Path Base\n",
    "base_path = r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors'\n",
    "\n",
    "# ARIMA Forecasting\n",
    "try:\n",
    "    # Automatically determine ARIMA order\n",
    "    auto_model = auto_arima(daily_footfall['transaction_id'].dropna(), seasonal=False, stepwise=True)\n",
    "    print(\"Auto ARIMA Model Summary:\")\n",
    "    print(auto_model.summary())\n",
    "\n",
    "    # Fit ARIMA model with the best parameters\n",
    "    arima_model = ARIMA(daily_footfall['transaction_id'], order=auto_model.order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "\n",
    "    # Adjust the forecast range based on the start and end dates provided\n",
    "    forecast_start_index = (start_date - daily_footfall.index[-1]).days  # Days from last data point to start date\n",
    "    arima_forecast = arima_model_fit.forecast(steps=(forecast_period + forecast_start_index))\n",
    "\n",
    "    # Creating a DataFrame for ARIMA Forecast\n",
    "    arima_forecast_dates = pd.date_range(daily_footfall.index[-1] + pd.Timedelta(days=1), periods=len(arima_forecast))\n",
    "    arima_forecast_df = pd.DataFrame({'timestamp': arima_forecast_dates, 'ARIMA_Forecast': arima_forecast})\n",
    "\n",
    "    # Filter the forecast to start exactly from the user-specified start date\n",
    "    arima_forecast_df = arima_forecast_df[arima_forecast_df['timestamp'] >= start_date]\n",
    "\n",
    "    # Display ARIMA Forecasted Data\n",
    "    print(f\"ARIMA Model - Forecasted Footfall from {start_date.date()} to {end_date.date()}:\")\n",
    "    print(arima_forecast_df)\n",
    "\n",
    "    # Aggregation of ARIMA Forecast\n",
    "    daily_forecast = arima_forecast_df.copy()\n",
    "    daily_forecast.set_index('timestamp', inplace=True)\n",
    "    weekly_forecast = daily_forecast.resample('W').sum()\n",
    "    monthly_forecast = daily_forecast.resample('M').sum()\n",
    "    yearly_forecast = daily_forecast.resample('Y').sum()\n",
    "    weekend_forecast = daily_forecast[daily_forecast.index.dayofweek >= 5].resample('D').sum()\n",
    "    weekday_forecast = daily_forecast[daily_forecast.index.dayofweek < 5].resample('D').sum()\n",
    "\n",
    "    # Add week of year and day of week to weekly forecast\n",
    "    weekly_forecast['week_of_year'] = weekly_forecast.index.isocalendar().week\n",
    "    weekly_forecast['day_of_week'] = weekly_forecast.index.day_name()\n",
    "\n",
    "    # Add month name to monthly forecast\n",
    "    monthly_forecast['month_name'] = monthly_forecast.index.month_name()\n",
    "\n",
    "    # Add day of week to weekend and weekday forecasts\n",
    "    weekend_forecast['day_of_week'] = weekend_forecast.index.day_name()\n",
    "    weekday_forecast['day_of_week'] = weekday_forecast.index.day_name()\n",
    "\n",
    "    # Save ARIMA Forecast to CSV\n",
    "    arima_forecast_df.to_csv(os.path.join(base_path, f'arima_forecast_{stn_no}_{eqn_no}.csv'), index=False)\n",
    "\n",
    "    # Save Aggregated ARIMA Forecast Data to CSV\n",
    "    weekly_forecast.to_csv(os.path.join(base_path, f'weekly_forecast_arima_{stn_no}_{eqn_no}.csv'))\n",
    "    monthly_forecast.to_csv(os.path.join(base_path, f'monthly_forecast_arima_{stn_no}_{eqn_no}.csv'))\n",
    "    yearly_forecast.to_csv(os.path.join(base_path, f'yearly_forecast_arima_{stn_no}_{eqn_no}.csv'))\n",
    "    weekend_forecast.to_csv(os.path.join(base_path, f'weekend_forecast_arima_{stn_no}_{eqn_no}.csv'))\n",
    "    weekday_forecast.to_csv(os.path.join(base_path, f'weekday_forecast_arima_{stn_no}_{eqn_no}.csv'))\n",
    "\n",
    "    # Plot ARIMA Forecast\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(daily_footfall.index, daily_footfall['transaction_id'], label='Observed', color='blue')\n",
    "    plt.plot(arima_forecast_df['timestamp'], arima_forecast_df['ARIMA_Forecast'], label='ARIMA Forecast', color='orange')\n",
    "    plt.title(f'ARIMA Model: Observed vs Forecasted Footfall ({start_date.date()} to {end_date.date()})')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Footfall')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred with ARIMA model: {e}\")\n",
    "\n",
    "# Prophet Model - Alternative Time-Series Forecasting\n",
    "daily_footfall_prophet = daily_footfall.reset_index()\n",
    "\n",
    "# Check the columns in the DataFrame\n",
    "print(daily_footfall_prophet.columns)  # This will help to understand the columns before renaming\n",
    "\n",
    "# Ensure the DataFrame only has 'timestamp' and 'transaction_id' after resetting the index\n",
    "if 'timestamp' in daily_footfall_prophet.columns and 'transaction_id' in daily_footfall_prophet.columns:\n",
    "    daily_footfall_prophet = daily_footfall_prophet[['timestamp', 'transaction_id']]  # Keep only necessary columns\n",
    "    daily_footfall_prophet.columns = ['ds', 'y']  # Rename columns for Prophet\n",
    "else:\n",
    "    raise ValueError(\"Expected columns 'timestamp' and 'transaction_id' not found in the DataFrame after resetting index\")\n",
    "\n",
    "# Convert 'ds' to datetime and 'y' to numeric\n",
    "daily_footfall_prophet['ds'] = pd.to_datetime(daily_footfall_prophet['ds'])\n",
    "daily_footfall_prophet['y'] = pd.to_numeric(daily_footfall_prophet['y'], errors='coerce')\n",
    "\n",
    "# Fit Prophet model\n",
    "prophet_model = Prophet()\n",
    "prophet_model.fit(daily_footfall_prophet)\n",
    "\n",
    "# Create future dataframe starting from the user-specified start_date\n",
    "future_df = pd.date_range(start=start_date, end=end_date)\n",
    "future_prophet = pd.DataFrame({'ds': future_df})\n",
    "forecast_prophet = prophet_model.predict(future_prophet)\n",
    "\n",
    "# Creating a DataFrame for Prophet Forecast\n",
    "prophet_forecast_df = forecast_prophet[['ds', 'yhat']]\n",
    "prophet_forecast_df.columns = ['timestamp', 'transaction_id']\n",
    "\n",
    "# Save Prophet Forecast to CSV\n",
    "prophet_forecast_df.to_csv(os.path.join(base_path, f'daily_footfall_forecast_prophet_{stn_no}_{eqn_no}.csv'), index=False)\n",
    "\n",
    "# Aggregation of Prophet Forecast\n",
    "daily_forecast_prophet = prophet_forecast_df.copy()\n",
    "daily_forecast_prophet.set_index('timestamp', inplace=True)\n",
    "weekly_forecast_prophet = daily_forecast_prophet.resample('W').sum()\n",
    "monthly_forecast_prophet = daily_forecast_prophet.resample('M').sum()\n",
    "yearly_forecast_prophet = daily_forecast_prophet.resample('Y').sum()\n",
    "weekend_forecast_prophet = daily_forecast_prophet[daily_forecast_prophet.index.dayofweek >= 5].resample('D').sum()\n",
    "weekday_forecast_prophet = daily_forecast_prophet[daily_forecast_prophet.index.dayofweek < 5].resample('D').sum()\n",
    "\n",
    "# Add week of year and day of week to weekly forecast\n",
    "weekly_forecast_prophet['week_of_year'] = weekly_forecast_prophet.index.isocalendar().week\n",
    "weekly_forecast_prophet['day_of_week'] = weekly_forecast_prophet.index.day_name()\n",
    "\n",
    "# Add month name to monthly forecast\n",
    "monthly_forecast_prophet['month_name'] = monthly_forecast_prophet.index.month_name()\n",
    "\n",
    "# Add day of week to weekend and weekday forecasts\n",
    "weekend_forecast_prophet['day_of_week'] = weekend_forecast_prophet.index.day_name()\n",
    "weekday_forecast_prophet['day_of_week'] = weekday_forecast_prophet.index.day_name()\n",
    "\n",
    "# Save Aggregated Forecast Data to CSV\n",
    "weekly_forecast_prophet.to_csv(os.path.join(base_path, f'weekly_forecast_prophet_{stn_no}_{eqn_no}.csv'))\n",
    "monthly_forecast_prophet.to_csv(os.path.join(base_path, f'monthly_forecast_prophet_{stn_no}_{eqn_no}.csv'))\n",
    "yearly_forecast_prophet.to_csv(os.path.join(base_path, f'yearly_forecast_prophet_{stn_no}_{eqn_no}.csv'))\n",
    "weekend_forecast_prophet.to_csv(os.path.join(base_path, f'weekend_forecast_prophet_{stn_no}_{eqn_no}.csv'))\n",
    "weekday_forecast_prophet.to_csv(os.path.join(base_path, f'weekday_forecast_prophet_{stn_no}_{eqn_no}.csv'))\n",
    "\n",
    "# Display the Prophet Forecast DataFrame\n",
    "print(prophet_forecast_df.head())\n",
    "\n",
    "print(\"Files saved to their respective destinations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9baa960-181a-4b68-8b98-197f68646bee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f83541-a321-4d31-b3a9-0b350aea9431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2274575a-beb1-4bd5-a06c-aad0a0454ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f1037c-a270-49bc-b456-01f372d7f74e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea89b3da-76bc-4485-98dc-404f2c218a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da41ba92-8792-481b-bbbd-6c7a963bb823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1d376c-50f3-44d5-a288-9a9f7ac55954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d246da-23a5-43f7-90c7-c99e239d3bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deeb4b12-40cc-4354-9ea3-7a3655547dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8468498-0588-474e-a711-e81ed54782ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf1c179-e381-4f91-9917-b31eb7c9e2b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8d919d-11d4-41ae-9409-81e1f943cd80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76dc8db-6c59-44ca-a1a7-32da010f40d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b204d247-a1dd-4227-b24b-fd4453973e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a1cb2b-9b9b-43d4-8d80-e4add70b3045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c891f9-772a-43f8-999f-cbda4f8d4469",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceef7639-d5e7-46b6-8e0a-83ae7dd4696e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b8fa5d-bfeb-4430-a164-cdc51aaf1eca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3964e5-a594-45ef-8aeb-f380be16914b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a223ab-0bf0-4d7e-8a25-f8a42808d9be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4f0f0b-a434-4fdd-9f6c-2bb00ef67719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc46be6-6076-4487-84bb-0514eddea193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cf2482-1933-4978-bd03-5bdd5558858d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd89dc71-8594-4686-94e2-0ec8cedcce1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ac0185-f10a-4972-9545-039214c70204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7046447f-f155-4798-aa5a-af9e83945bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0341d54a-a32d-4e4a-9752-f2f579483c1d",
   "metadata": {},
   "source": [
    "HOURLY FORECAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5863e7-ca69-4b78-b58e-0ebdc0c78984",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from pmdarima import auto_arima\n",
    "from prophet import Prophet\n",
    "import numpy as np\n",
    "\n",
    "# Load Dataset\n",
    "df = pd.read_csv('transaction.csv')  # Replace with your dataset path\n",
    "\n",
    "# Data Preprocessing\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Input: Define Start and End Date for Filtering\n",
    "start_date = '2024-09-01 00:00:00'  # Replace with your start date\n",
    "end_date = '2024-09-30 11:57:00'    # Replace with your end date\n",
    "\n",
    "# Filter data based on the specified date range\n",
    "df_filtered = df.loc[start_date:end_date]\n",
    "\n",
    "# Aggregating the Footfall at an Hourly Level\n",
    "hourly_footfall = df_filtered.resample('H').count()  # Hourly footfall\n",
    "\n",
    "# Grouping by hour (0 to 23) to get the sum of transactions for each hour across all days\n",
    "hourly_sums = hourly_footfall.groupby(hourly_footfall.index.hour).sum()\n",
    "\n",
    "# Exploratory Data Analysis (EDA) - Visualizing Trends\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(hourly_sums.index, hourly_sums['transaction_id'], label='Sum of Transactions per Hour', color='blue')\n",
    "plt.title(f'Sum of Transactions for Each Hour Across {start_date} to {end_date}')\n",
    "plt.xlabel('Hour of the Day')\n",
    "plt.ylabel('Sum of Transactions')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Stationarity Check using Augmented Dickey-Fuller Test\n",
    "result = adfuller(hourly_sums['transaction_id'])\n",
    "print(f'ADF Statistic: {result[0]}')\n",
    "print(f'p-value: {result[1]}')\n",
    "\n",
    "# Ensure there are enough observations (at least 24) for decomposition\n",
    "if len(hourly_sums) >= 24:\n",
    "    # Time-Series Decomposition\n",
    "    decomposition = seasonal_decompose(hourly_sums['transaction_id'], model='additive', period=12)\n",
    "    fig = decomposition.plot()\n",
    "    fig.suptitle('Time-Series Decomposition: Trend, Seasonal, and Residuals (Hourly Sum)', y=1.02)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Not enough data points for decomposition. Required: 24, Found: {len(hourly_sums)}\")\n",
    "\n",
    "# Use the filtered dataset for training the ARIMA model\n",
    "try:\n",
    "    # Automatically determine ARIMA order\n",
    "    auto_model = auto_arima(hourly_sums['transaction_id'].dropna(), seasonal=False, stepwise=True)\n",
    "    print(\"Auto ARIMA Model Summary:\")\n",
    "    print(auto_model.summary())\n",
    "\n",
    "    # Fit ARIMA model with the best parameters\n",
    "    arima_model = ARIMA(hourly_sums['transaction_id'], order=auto_model.order)\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    \n",
    "    # Forecasting the next 24 hours (1 day)\n",
    "    arima_forecast = arima_model_fit.forecast(steps=24)\n",
    "    \n",
    "    # Creating a DataFrame for ARIMA Forecast\n",
    "    arima_forecast_df = pd.DataFrame({'hour': np.arange(0, 24), \n",
    "                                      'ARIMA_Forecast': arima_forecast})\n",
    "\n",
    "    # Display ARIMA Forecasted Data\n",
    "    print(\"ARIMA Model - 24 Hour Forecasted Sum of Transactions:\")\n",
    "    print(arima_forecast_df)\n",
    "\n",
    "    # Plotting ARIMA Forecast\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(hourly_sums.index, hourly_sums['transaction_id'], label='Observed Sum of Transactions', color='blue')\n",
    "    plt.plot(arima_forecast_df['hour'], arima_forecast_df['ARIMA_Forecast'], label='ARIMA Forecast', color='orange')\n",
    "    plt.title('ARIMA Model: Observed vs Forecasted Sum of Transactions for Each Hour')\n",
    "    plt.xlabel('Hour of the Day')\n",
    "    plt.ylabel('Sum of Transactions')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred with ARIMA model: {e}\")\n",
    "\n",
    "# Prophet Model - Alternative Time-Series Forecasting\n",
    "hourly_sums_prophet = hourly_sums.reset_index()\n",
    "\n",
    "# Drop any additional columns and rename correctly\n",
    "hourly_sums_prophet.columns = ['ds', 'y']  # Rename for Prophet\n",
    "\n",
    "# Ensure 'ds' is numeric (hour of the day) and 'y' is the sum of transactions\n",
    "hourly_sums_prophet['ds'] = hourly_sums_prophet['ds'].astype(int)\n",
    "hourly_sums_prophet['y'] = pd.to_numeric(hourly_sums_prophet['y'], errors='coerce')\n",
    "\n",
    "# Fit Prophet model\n",
    "prophet_model = Prophet()\n",
    "prophet_model.fit(hourly_sums_prophet)\n",
    "future = prophet_model.make_future_dataframe(periods=24, freq='H')  # Forecast next 24 hours\n",
    "forecast_prophet = prophet_model.predict(future)\n",
    "\n",
    "# Creating a DataFrame for Prophet Forecast\n",
    "prophet_forecast_df = forecast_prophet[['ds', 'yhat']].tail(24)\n",
    "prophet_forecast_df.columns = ['hour', 'Prophet_Forecast']\n",
    "\n",
    "# Convert 'hour' column in prophet_forecast_df to integer format\n",
    "prophet_forecast_df['hour'] = prophet_forecast_df['hour'].dt.hour\n",
    "\n",
    "# Ensure 'hour' column in arima_forecast_df is integer\n",
    "arima_forecast_df['hour'] = arima_forecast_df['hour'].astype(int)\n",
    "\n",
    "# Merging ARIMA and Prophet Forecasts\n",
    "forecast_df = pd.merge(arima_forecast_df, prophet_forecast_df, on='hour', how='outer')\n",
    "\n",
    "# Save to CSV file\n",
    "forecast_df.to_csv(r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors\\hourly_forecasted_sum_transactions.csv', index=False)\n",
    "\n",
    "# Display the merged forecast DataFrame\n",
    "print(forecast_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e2a539-c600-4b80-b50f-63c61575d150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Load Dataset\n",
    "df = pd.read_csv('transaction.csv')  # Replace with your dataset path\n",
    "\n",
    "# Data Preprocessing\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Input: Define Start and End Date for Filtering\n",
    "start_date = '2024-09-01 00:00:00'  # Replace with your start date\n",
    "end_date = '2024-09-30 11:57:00'    # Replace with your end date\n",
    "\n",
    "# Filter data based on the specified date range\n",
    "df_filtered = df.loc[start_date:end_date]\n",
    "\n",
    "# Aggregating the Footfall at an Hourly Level\n",
    "hourly_footfall = df_filtered.resample('H').count()  # Hourly footfall\n",
    "\n",
    "# Grouping by hour (0 to 23) to get the sum of transactions for each hour across all days\n",
    "hourly_sums = hourly_footfall.groupby(hourly_footfall.index.hour).sum()\n",
    "\n",
    "# --- Train-Test Split ---\n",
    "# Use 80% of the data for training and the last 20% for testing\n",
    "split_index = int(0.8 * len(hourly_sums))\n",
    "train_data = hourly_sums.iloc[:split_index]\n",
    "test_data = hourly_sums.iloc[split_index:]\n",
    "\n",
    "# --- ARIMA Forecast ---\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from pmdarima import auto_arima\n",
    "\n",
    "# Automatically determine ARIMA order using training data\n",
    "auto_model = auto_arima(train_data['transaction_id'].dropna(), seasonal=False, stepwise=True)\n",
    "\n",
    "# Fit ARIMA model with the best parameters on train data\n",
    "arima_model = ARIMA(train_data['transaction_id'], order=auto_model.order)\n",
    "arima_model_fit = arima_model.fit()\n",
    "\n",
    "# Forecasting the next len(test_data) hours\n",
    "arima_forecast = arima_model_fit.forecast(steps=len(test_data))\n",
    "\n",
    "# Plot ARIMA Forecast vs Actual\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(test_data.index, test_data['transaction_id'], label='Actual', color='blue')\n",
    "plt.plot(test_data.index, arima_forecast, label='ARIMA Forecast', color='orange')\n",
    "plt.title('ARIMA Forecast vs Actual')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Sum of Transactions')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Compute MAE and RMSE for ARIMA\n",
    "mae_arima = mean_absolute_error(test_data['transaction_id'], arima_forecast)\n",
    "rmse_arima = np.sqrt(mean_squared_error(test_data['transaction_id'], arima_forecast))\n",
    "\n",
    "print(f'ARIMA Model MAE: {mae_arima}')\n",
    "print(f'ARIMA Model RMSE: {rmse_arima}')\n",
    "\n",
    "\n",
    "# --- Prophet Forecast ---\n",
    "from prophet import Prophet\n",
    "\n",
    "# Prepare data for Prophet\n",
    "hourly_sums_prophet = hourly_sums.reset_index()\n",
    "hourly_sums_prophet.columns = ['ds', 'y']  # Rename for Prophet\n",
    "\n",
    "# Train-Test Split for Prophet\n",
    "train_prophet = hourly_sums_prophet.iloc[:split_index]\n",
    "test_prophet = hourly_sums_prophet.iloc[split_index:]\n",
    "\n",
    "# Fit Prophet model on training data\n",
    "prophet_model = Prophet()\n",
    "prophet_model.fit(train_prophet)\n",
    "\n",
    "# Make future dataframe for test period\n",
    "future = prophet_model.make_future_dataframe(periods=len(test_prophet), freq='H')\n",
    "forecast_prophet = prophet_model.predict(future)\n",
    "\n",
    "# Get only the predicted values for the test period\n",
    "prophet_forecast = forecast_prophet[['ds', 'yhat']].tail(len(test_prophet))['yhat']\n",
    "\n",
    "# Plot Prophet Forecast vs Actual\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(test_prophet['ds'], test_prophet['y'], label='Actual', color='blue')\n",
    "plt.plot(test_prophet['ds'], prophet_forecast, label='Prophet Forecast', color='green')\n",
    "plt.title('Prophet Forecast vs Actual')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Sum of Transactions')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Compute MAE and RMSE for Prophet\n",
    "mae_prophet = mean_absolute_error(test_prophet['y'], prophet_forecast)\n",
    "rmse_prophet = np.sqrt(mean_squared_error(test_prophet['y'], prophet_forecast))\n",
    "\n",
    "print(f'Prophet Model MAE: {mae_prophet}')\n",
    "print(f'Prophet Model RMSE: {rmse_prophet}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbec167-0316-4c0b-88dd-b502b83857aa",
   "metadata": {},
   "source": [
    "checking model fitting on hours "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0cc2e2-79d9-4e4e-aabb-5283e72d037b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from math import sqrt\n",
    "import xgboost as xgb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('transaction_updated.csv')\n",
    "print(df.shape)\n",
    "\n",
    "# Define the specific format for your datetime strings\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], format='%Y-%m-%dT%H-%M-%S.%fZ', errors='coerce')\n",
    "\n",
    "# Check for any failed parsing\n",
    "print(f\"Number of missing timestamps: {df['timestamp'].isnull().sum()}\")\n",
    "\n",
    "# Drop rows where the timestamp could not be parsed\n",
    "df = df.dropna(subset=['timestamp'])\n",
    "\n",
    "# Ensure the 'transaction_id' column is numeric\n",
    "df['transaction_id'] = pd.to_numeric(df['transaction_id'], errors='coerce')\n",
    "\n",
    "# Add lag features\n",
    "for lag in range(1, 25):\n",
    "    df[f'lag_{lag}'] = df['transaction_id'].shift(lag)\n",
    "\n",
    "# Fill NaN values\n",
    "df.fillna(method='bfill', inplace=True)\n",
    "\n",
    "# Set the timestamp as the index\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Check if there is data before resampling\n",
    "print(f\"Data before resampling: {df.shape}\")\n",
    "print(df.head())\n",
    "\n",
    "# Resample to hourly data\n",
    "df = df.resample('H').sum()\n",
    "\n",
    "# Check if data is empty after resampling\n",
    "if df.empty:\n",
    "    raise ValueError(\"Resampled data is empty. Check your resampling frequency or original data.\")\n",
    "\n",
    "print(f\"Data shape after resampling: {df.shape}\")\n",
    "print(df.head())\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_size = int(len(df) * 0.8)\n",
    "train, test = df.iloc[:train_size], df.iloc[train_size:]\n",
    "\n",
    "# Print shapes and heads of train and test data\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"Train head: {train.head()}\")\n",
    "print(f\"Test head: {test.head()}\")\n",
    "\n",
    "# Create function to calculate RMSE and MAE\n",
    "def evaluate_model(predictions, true_values):\n",
    "    mae = mean_absolute_error(true_values, predictions)\n",
    "    rmse = sqrt(mean_squared_error(true_values, predictions))\n",
    "    return mae, rmse\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "# 1. ARIMA\n",
    "try:\n",
    "    if len(train) > 0:\n",
    "        arima_model = ARIMA(train['transaction_id'], order=(5,1,0))\n",
    "        arima_fit = arima_model.fit()\n",
    "        arima_pred = arima_fit.forecast(len(test))\n",
    "        mae, rmse = evaluate_model(arima_pred, test['transaction_id'])\n",
    "        results['ARIMA'] = (mae, rmse)\n",
    "    else:\n",
    "        print(\"ARIMA Error: Not enough data to fit the model.\")\n",
    "except Exception as e:\n",
    "    print(f\"ARIMA Error: {e}\")\n",
    "\n",
    "# 2. SARIMA\n",
    "try:\n",
    "    if len(train) > 0:\n",
    "        sarima_model = SARIMAX(train['transaction_id'], order=(1,1,1), seasonal_order=(1,1,1,24))\n",
    "        sarima_fit = sarima_model.fit(disp=False)\n",
    "        sarima_pred = sarima_fit.forecast(len(test))\n",
    "        mae, rmse = evaluate_model(sarima_pred, test['transaction_id'])\n",
    "        results['SARIMA'] = (mae, rmse)\n",
    "    else:\n",
    "        print(\"SARIMA Error: Not enough data to fit the model.\")\n",
    "except Exception as e:\n",
    "    print(f\"SARIMA Error: {e}\")\n",
    "\n",
    "# 3. Exponential Smoothing (ETS)\n",
    "try:\n",
    "    if len(train) > 0:\n",
    "        ets_model = ExponentialSmoothing(train['transaction_id'], seasonal='add', seasonal_periods=24).fit()\n",
    "        ets_pred = ets_model.forecast(len(test))\n",
    "        mae, rmse = evaluate_model(ets_pred, test['transaction_id'])\n",
    "        results['ETS'] = (mae, rmse)\n",
    "    else:\n",
    "        print(\"ETS Error: Not enough data to fit the model.\")\n",
    "except Exception as e:\n",
    "    print(f\"ETS Error: {e}\")\n",
    "\n",
    "# 4. Facebook Prophet\n",
    "try:\n",
    "    prophet_df = df.reset_index().rename(columns={'timestamp': 'ds', 'transaction_id': 'y'})\n",
    "    train_prophet = prophet_df[:train_size]\n",
    "    test_prophet = prophet_df[train_size:]\n",
    "\n",
    "    if len(train_prophet) > 1:  # Prophet needs at least 2 rows\n",
    "        prophet_model = Prophet()\n",
    "        prophet_model.fit(train_prophet)\n",
    "        future = prophet_model.make_future_dataframe(periods=len(test_prophet), freq='H')\n",
    "        prophet_forecast = prophet_model.predict(future)\n",
    "        prophet_pred = prophet_forecast['yhat'].iloc[-len(test):].values\n",
    "        mae, rmse = evaluate_model(prophet_pred, test['transaction_id'].values)\n",
    "        results['Prophet'] = (mae, rmse)\n",
    "    else:\n",
    "        print(\"Prophet Error: Not enough data to fit the model.\")\n",
    "except Exception as e:\n",
    "    print(f\"Prophet Error: {e}\")\n",
    "\n",
    "# 5. LSTM\n",
    "# Prepare the data for LSTM\n",
    "try:\n",
    "    if len(train) > 24:  # Ensure there is enough data for LSTM and TimeseriesGenerator\n",
    "        scaler = MinMaxScaler()\n",
    "        train_scaled = scaler.fit_transform(train[['transaction_id']])\n",
    "        test_scaled = scaler.transform(test[['transaction_id']])\n",
    "\n",
    "        train_gen = TimeseriesGenerator(train_scaled, train_scaled, length=24, batch_size=1)\n",
    "        test_gen = TimeseriesGenerator(test_scaled, test_scaled, length=24, batch_size=1)\n",
    "\n",
    "        lstm_model = Sequential()\n",
    "        lstm_model.add(LSTM(50, activation='relu', input_shape=(24, 1)))\n",
    "        lstm_model.add(Dense(1))\n",
    "        lstm_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "        lstm_model.fit(train_gen, epochs=10, verbose=1)\n",
    "        lstm_pred = lstm_model.predict(test_gen)\n",
    "        lstm_pred_rescaled = scaler.inverse_transform(lstm_pred)\n",
    "        mae, rmse = evaluate_model(lstm_pred_rescaled, test['transaction_id'].values[24:])\n",
    "        results['LSTM'] = (mae, rmse)\n",
    "    else:\n",
    "        print(\"LSTM Error: Not enough data to fit the model.\")\n",
    "except Exception as e:\n",
    "    print(f\"LSTM Error: {e}\")\n",
    "\n",
    "# 6. XGBoost\n",
    "try:\n",
    "    if len(train) > 24:  # Ensure there is enough data for XGBoost\n",
    "        # Create lag features for XGBoost\n",
    "        train_xgb = train.dropna()\n",
    "        test_xgb = test.dropna()\n",
    "\n",
    "        X_train = train_xgb.drop('transaction_id', axis=1)\n",
    "        y_train = train_xgb['transaction_id']\n",
    "        X_test = test_xgb.drop('transaction_id', axis=1)\n",
    "        y_test = test_xgb['transaction_id']\n",
    "\n",
    "        # Hyperparameter tuning with GridSearchCV\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [6, 10],\n",
    "            'learning_rate': [0.01, 0.1],\n",
    "        }\n",
    "        xgb_model = xgb.XGBRegressor()\n",
    "        grid_search = GridSearchCV(xgb_model, param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        xgb_pred = best_model.predict(X_test)\n",
    "        mae, rmse = evaluate_model(xgb_pred, y_test)\n",
    "        results['XGBoost'] = (mae, rmse)\n",
    "    else:\n",
    "        print(\"XGBoost Error: Not enough data to fit the model.\")\n",
    "except Exception as e:\n",
    "    print(f\"XGBoost Error: {e}\")\n",
    "\n",
    "# Print the results\n",
    "for model_name, (mae, rmse) in results.items():\n",
    "    print(f'{model_name} - MAE: {mae}, RMSE: {rmse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1d43ab-190a-4a82-9af6-87361f521dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b1be253-b1b1-4d35-b7b0-c98b65dfd37d",
   "metadata": {},
   "source": [
    "whole model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec78819-3dad-40d8-907f-ca4b88b4bb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from pmdarima import auto_arima\n",
    "from prophet import Prophet\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load Dataset\n",
    "df = pd.read_csv('transaction.csv')  # Update path to the uploaded file\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Manually define holidays and festivals\n",
    "holidays = pd.to_datetime(['2024-01-26', '2024-08-15', '2024-10-02'])  # Replace with real holiday dates\n",
    "festivals = pd.to_datetime(['2024-03-29', '2024-11-04', '2024-12-25'])  # Replace with real festival dates\n",
    "\n",
    "# Function to aggregate data based on frequency\n",
    "def aggregate_data(df, freq):\n",
    "    return df.resample(freq).count()\n",
    "\n",
    "# ARIMA Forecast Function\n",
    "def arima_forecast(data, periods):\n",
    "    if data.empty:\n",
    "        print(\"ARIMA Error: No data available for ARIMA model.\")\n",
    "        return None\n",
    "    try:\n",
    "        data_series = data.squeeze()  # Ensure data is a Series\n",
    "        model = auto_arima(data_series, seasonal=False, stepwise=True)\n",
    "        arima_model = ARIMA(data_series, order=model.order)\n",
    "        arima_model_fit = arima_model.fit()\n",
    "        forecast = arima_model_fit.forecast(steps=periods)\n",
    "        forecast_df = pd.DataFrame({'timestamp': pd.date_range(start=data.index[-1], periods=periods + 1, freq='D')[1:], \n",
    "                                    'ARIMA_Forecast': forecast})\n",
    "        return forecast_df\n",
    "    except Exception as e:\n",
    "        print(f\"ARIMA Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Prophet Forecast Function\n",
    "def prophet_forecast(data, periods):\n",
    "    if data.empty or data.dropna().shape[0] < 2:\n",
    "        print(\"Prophet Error: Not enough data available for Prophet model.\")\n",
    "        return None\n",
    "    try:\n",
    "        data_prophet = data.reset_index()\n",
    "        data_prophet.columns = ['ds', 'y']\n",
    "        model = Prophet()\n",
    "        model.fit(data_prophet)\n",
    "        future = model.make_future_dataframe(periods=periods)\n",
    "        forecast = model.predict(future)\n",
    "        forecast_df = forecast[['ds', 'yhat']].tail(periods)\n",
    "        forecast_df.columns = ['timestamp', 'Prophet_Forecast']\n",
    "        return forecast_df\n",
    "    except Exception as e:\n",
    "        print(f\"Prophet Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Helper function to combine ARIMA and Prophet forecasts\n",
    "def forecast_combined(data, periods):\n",
    "    if 'transaction_id' not in data.columns:\n",
    "        print(\"Error: 'transaction_id' column is missing in the data.\")\n",
    "        return None\n",
    "    \n",
    "    arima_forecast_df = arima_forecast(data['transaction_id'], periods)\n",
    "    prophet_forecast_df = prophet_forecast(data['transaction_id'], periods)\n",
    "\n",
    "    if arima_forecast_df is not None and prophet_forecast_df is not None:\n",
    "        combined_forecast_df = pd.merge(arima_forecast_df, prophet_forecast_df, on='timestamp', how='outer')\n",
    "    else:\n",
    "        combined_forecast_df = None\n",
    "    \n",
    "    return combined_forecast_df\n",
    "\n",
    "# Forecast functions with date range filtering\n",
    "def forecast_hourly(df, start_date, end_date, periods):\n",
    "    data = df[start_date:end_date]\n",
    "    if data.empty:\n",
    "        print(\"No data available for the specified hourly range.\")\n",
    "        return None\n",
    "    data = aggregate_data(data, 'H')\n",
    "    return forecast_combined(data, periods)\n",
    "\n",
    "def forecast_daily(df, start_date, end_date, periods):\n",
    "    data = df[start_date:end_date]\n",
    "    if data.empty:\n",
    "        print(\"No data available for the specified daily range.\")\n",
    "        return None\n",
    "    data = aggregate_data(data, 'D')\n",
    "    return forecast_combined(data, periods)\n",
    "\n",
    "def forecast_weekly(df, start_date, end_date, periods):\n",
    "    data = df[start_date:end_date]\n",
    "    if data.empty:\n",
    "        print(\"No data available for the specified weekly range.\")\n",
    "        return None\n",
    "    data = aggregate_data(data, 'W')\n",
    "    return forecast_combined(data, periods)\n",
    "\n",
    "def forecast_monthly(df, start_date, end_date, periods):\n",
    "    data = df[start_date:end_date]\n",
    "    if data.empty:\n",
    "        print(\"No data available for the specified monthly range.\")\n",
    "        return None\n",
    "    data = aggregate_data(data, 'M')\n",
    "    return forecast_combined(data, periods)\n",
    "\n",
    "def forecast_yearly(df, start_date, end_date, periods):\n",
    "    data = df[start_date:end_date]\n",
    "    if data.empty:\n",
    "        print(\"No data available for the specified yearly range.\")\n",
    "        return None\n",
    "    data = aggregate_data(data, 'Y')\n",
    "    return forecast_combined(data, periods)\n",
    "\n",
    "def forecast_festival(df, periods):\n",
    "    data = df[df.index.isin(festivals)]\n",
    "    if data.empty:\n",
    "        print(\"No data available for the specified festivals.\")\n",
    "        return None\n",
    "    return forecast_combined(data, periods)\n",
    "\n",
    "def forecast_holiday(df, periods):\n",
    "    data = df[df.index.isin(holidays)]\n",
    "    if data.empty:\n",
    "        print(\"No data available for the specified holidays.\")\n",
    "        return None\n",
    "    return forecast_combined(data, periods)\n",
    "\n",
    "# Forecast function for transactions per hour of the day\n",
    "def forecast_hourly_by_hour(df, periods):\n",
    "    forecast_list = []\n",
    "    for hour in range(24):  # Loop through each hour of the day\n",
    "        hour_data = df[df.index.hour == hour]\n",
    "        if hour_data.empty:\n",
    "            print(f\"No data available for the {hour+1}th hour.\")\n",
    "            continue\n",
    "        hour_data = aggregate_data(hour_data, 'D')  # Aggregate by day\n",
    "        forecast = forecast_combined(hour_data, periods)\n",
    "        if forecast is not None:\n",
    "            forecast['hour'] = hour + 1  # Add the hour identifier\n",
    "            forecast_list.append(forecast)\n",
    "\n",
    "    if forecast_list:\n",
    "        forecast_df = pd.concat(forecast_list)\n",
    "        return forecast_df\n",
    "    else:\n",
    "        print(\"Forecasting failed for all hours.\")\n",
    "        return None\n",
    "\n",
    "# Example usage for forecast with different types\n",
    "forecast_type = 'hourly_by_hour'  # 'hourly', 'daily', 'weekly', 'monthly', 'yearly', 'festival', 'holiday', 'hourly_by_hour'\n",
    "start_date = '2024-01-01'  # Example start date\n",
    "end_date = '2024-12-31'    # Example end date\n",
    "periods = 30  # Number of periods to forecast\n",
    "\n",
    "if forecast_type == 'hourly':\n",
    "    forecast = forecast_hourly(df, start_date, end_date, periods)\n",
    "elif forecast_type == 'daily':\n",
    "    forecast = forecast_daily(df, start_date, end_date, periods)\n",
    "elif forecast_type == 'weekly':\n",
    "    forecast = forecast_weekly(df, start_date, end_date, periods)\n",
    "elif forecast_type == 'monthly':\n",
    "    forecast = forecast_monthly(df, start_date, end_date, periods)\n",
    "elif forecast_type == 'yearly':\n",
    "    forecast = forecast_yearly(df, start_date, end_date, periods)\n",
    "elif forecast_type == 'festival':\n",
    "    forecast = forecast_festival(df, periods)\n",
    "elif forecast_type == 'holiday':\n",
    "    forecast = forecast_holiday(df, periods)\n",
    "elif forecast_type == 'hourly_by_hour':\n",
    "    forecast = forecast_hourly_by_hour(df, periods)\n",
    "\n",
    "# Save the forecasted data to CSV\n",
    "output_dir = r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_dir, f'{forecast_type}_forecast.csv')\n",
    "\n",
    "if forecast is not None:\n",
    "    forecast.to_csv(output_path, index=False)\n",
    "    print(f\"Forecast saved at: {output_path}\")\n",
    "else:\n",
    "    print(\"Forecasting failed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6317a73a-623b-47a5-a82e-baf1fedb806f",
   "metadata": {},
   "source": [
    "BASED ON THE NEW DATA FROM OFFICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18c7ff7-52bf-4ebb-9fa4-ac28c7acfe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from prophet import Prophet\n",
    "import os\n",
    "\n",
    "# Load Dataset\n",
    "df = pd.read_csv('transaction.csv')  # Replace with your dataset path\n",
    "\n",
    "# Data Preprocessing\n",
    "# Adjusted to handle the format with 'Z' at the end (UTC time)\n",
    "df['Dt'] = pd.to_datetime(df['Dt'], errors='coerce')\n",
    "\n",
    "# Check for any rows where parsing failed\n",
    "invalid_dates = df[df['Dt'].isnull()]\n",
    "if not invalid_dates.empty:\n",
    "    print(\"Some dates could not be parsed:\")\n",
    "    print(invalid_dates)\n",
    "\n",
    "# Drop rows with invalid dates if needed\n",
    "df = df.dropna(subset=['Dt'])\n",
    "\n",
    "# Continue with setting the index\n",
    "df.set_index('Dt', inplace=True)\n",
    "\n",
    "# Filtering based on user input for station number and equipment number\n",
    "stn_no = input(\"Enter station number (or 'all' for all stations): \")\n",
    "eqn_no = input(\"Enter equipment number (or 'all' for all equipment): \")\n",
    "\n",
    "# Apply filters based on the input\n",
    "if stn_no.lower() != 'all':\n",
    "    df = df[df['Sta'] == int(stn_no)]\n",
    "if eqn_no.lower() != 'all':\n",
    "    df = df[df['EqN'] == int(eqn_no)]\n",
    "\n",
    "# Aggregating the Footfall at Hourly Level\n",
    "hourly_footfall = df.resample('H').count()  # Hourly footfall\n",
    "\n",
    "# Set frequency for hourly_footfall\n",
    "hourly_footfall = hourly_footfall.asfreq('H')\n",
    "\n",
    "# Time-Series Decomposition for visual inspection\n",
    "decomposition = seasonal_decompose(hourly_footfall['_id'], model='additive')\n",
    "fig = decomposition.plot()\n",
    "fig.suptitle('Time-Series Decomposition: Trend, Seasonal, and Residuals', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Specify Forecast Period\n",
    "start_date = pd.to_datetime(input(\"Enter forecast start date (YYYY-MM-DD): \"))\n",
    "end_date = pd.to_datetime(input(\"Enter forecast end date (YYYY-MM-DD): \"))\n",
    "forecast_period = (end_date - start_date).days * 24  # Forecast period in hours\n",
    "\n",
    "# File Path Base\n",
    "base_path = r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors'\n",
    "\n",
    "# Prophet Model - Time-Series Forecasting with Seasonalities\n",
    "hourly_footfall_prophet = hourly_footfall.reset_index()\n",
    "\n",
    "# Ensure the DataFrame only has 'timestamp' and 'transaction_id' after resetting the index\n",
    "hourly_footfall_prophet = hourly_footfall_prophet[['Dt', '_id']]\n",
    "hourly_footfall_prophet.columns = ['ds', 'y']  # Rename columns for Prophet\n",
    "\n",
    "# Remove timezone information from the 'ds' column\n",
    "hourly_footfall_prophet['ds'] = hourly_footfall_prophet['ds'].dt.tz_localize(None)\n",
    "\n",
    "# Convert 'y' to numeric\n",
    "hourly_footfall_prophet['y'] = pd.to_numeric(hourly_footfall_prophet['y'], errors='coerce')\n",
    "\n",
    "# Initialize Prophet model with specific seasonalities\n",
    "prophet_model = Prophet(\n",
    "    yearly_seasonality=False,  # Disable built-in yearly seasonality to customize it\n",
    "    weekly_seasonality=False,  # Disable built-in weekly seasonality\n",
    "    daily_seasonality=False    # Disable built-in daily seasonality\n",
    ")\n",
    "\n",
    "# Add custom seasonalities based on decomposition insights\n",
    "prophet_model.add_seasonality(name='daily', period=1, fourier_order=10)\n",
    "prophet_model.add_seasonality(name='weekly', period=7, fourier_order=5)\n",
    "prophet_model.add_seasonality(name='yearly', period=365.25, fourier_order=10)\n",
    "\n",
    "# Fit the Prophet model\n",
    "prophet_model.fit(hourly_footfall_prophet)\n",
    "\n",
    "# Create future dataframe starting from the user-specified start_date\n",
    "future_df = pd.date_range(start=start_date, end=end_date, freq='H')\n",
    "future_prophet = pd.DataFrame({'ds': future_df})\n",
    "\n",
    "# Forecast with Prophet model\n",
    "forecast_prophet = prophet_model.predict(future_prophet)\n",
    "\n",
    "# Creating a DataFrame for Prophet Forecast\n",
    "prophet_forecast_df = forecast_prophet[['ds', 'yhat']]\n",
    "prophet_forecast_df.columns = ['Dt', '_id']\n",
    "\n",
    "# Round off the Prophet forecasted data\n",
    "prophet_forecast_df['_id'] = prophet_forecast_df['_id'].round()\n",
    "\n",
    "# Save Prophet Forecast to CSV\n",
    "prophet_forecast_df.to_csv(os.path.join(base_path, f'hourly_footfall_forecast_prophet_{stn_no}_{eqn_no}.csv'), index=False)\n",
    "\n",
    "# Aggregation of Prophet Forecast\n",
    "daily_forecast_prophet = prophet_forecast_df.copy()\n",
    "daily_forecast_prophet.set_index('Dt', inplace=True)\n",
    "\n",
    "# Aggregation: Daily, Weekly, Monthly, Yearly\n",
    "daily_forecast_prophet_resampled = daily_forecast_prophet.resample('D').sum()\n",
    "weekly_forecast_prophet_resampled = daily_forecast_prophet.resample('W').sum()\n",
    "monthly_forecast_prophet_resampled = daily_forecast_prophet.resample('M').sum()\n",
    "yearly_forecast_prophet_resampled = daily_forecast_prophet.resample('Y').sum()\n",
    "\n",
    "# Weekend and Weekday Aggregation\n",
    "weekend_forecast_prophet_resampled = daily_forecast_prophet[daily_forecast_prophet.index.dayofweek >= 5].resample('D').sum()\n",
    "weekday_forecast_prophet_resampled = daily_forecast_prophet[daily_forecast_prophet.index.dayofweek < 5].resample('D').sum()\n",
    "\n",
    "# Add week of year and day of week to weekly forecast\n",
    "weekly_forecast_prophet_resampled['week_of_year'] = weekly_forecast_prophet_resampled.index.isocalendar().week\n",
    "weekly_forecast_prophet_resampled['day_of_week'] = weekly_forecast_prophet_resampled.index.day_name()\n",
    "\n",
    "# Add month name to monthly forecast\n",
    "monthly_forecast_prophet_resampled['month_name'] = monthly_forecast_prophet_resampled.index.month_name()\n",
    "\n",
    "# Add day of week to weekend and weekday forecasts\n",
    "weekend_forecast_prophet_resampled['day_of_week'] = weekend_forecast_prophet_resampled.index.day_name()\n",
    "weekday_forecast_prophet_resampled['day_of_week'] = weekday_forecast_prophet_resampled.index.day_name()\n",
    "\n",
    "# Save Aggregated Forecast Data to CSV\n",
    "daily_forecast_prophet_resampled.to_csv(os.path.join(base_path, f'daily_forecast_prophet_{stn_no}_{eqn_no}.csv'))\n",
    "weekly_forecast_prophet_resampled.to_csv(os.path.join(base_path, f'weekly_forecast_prophet_{stn_no}_{eqn_no}.csv'))\n",
    "monthly_forecast_prophet_resampled.to_csv(os.path.join(base_path, f'monthly_forecast_prophet_{stn_no}_{eqn_no}.csv'))\n",
    "yearly_forecast_prophet_resampled.to_csv(os.path.join(base_path, f'yearly_forecast_prophet_{stn_no}_{eqn_no}.csv'))\n",
    "weekend_forecast_prophet_resampled.to_csv(os.path.join(base_path, f'weekend_forecast_prophet_{stn_no}_{eqn_no}.csv'))\n",
    "weekday_forecast_prophet_resampled.to_csv(os.path.join(base_path, f'weekday_forecast_prophet_{stn_no}_{eqn_no}.csv'))\n",
    "\n",
    "# Aggregation of Prophet Forecast Data by Hour of Day\n",
    "prophet_forecast_df['hour'] = prophet_forecast_df['Dt'].dt.hour\n",
    "hourly_aggr_prophet = prophet_forecast_df.groupby('hour')['_id'].sum().reset_index()\n",
    "\n",
    "# Save Aggregated Hourly Prophet Forecast Data to CSV\n",
    "hourly_aggr_prophet.to_csv(os.path.join(base_path, f'hourly_aggr_prophet_forecast_{stn_no}_{eqn_no}.csv'), index=False)\n",
    "\n",
    "# Display the Prophet Forecast DataFrame\n",
    "print(prophet_forecast_df.head())\n",
    "\n",
    "print(\"Files saved to their respective destinations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7281ec7-84d8-4b1c-9c22-d3cf277cdb83",
   "metadata": {},
   "source": [
    "FORECAST VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921e1d09-19f8-4a54-8e07-310dd34d57e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the forecasted data for validation\n",
    "forecast_prophet_df = pd.read_csv(os.path.join(base_path, f'hourly_footfall_forecast_prophet_{stn_no}_{eqn_no}.csv'))\n",
    "\n",
    "# Convert 'Dt' back to datetime\n",
    "forecast_prophet_df['Dt'] = pd.to_datetime(forecast_prophet_df['Dt'])\n",
    "\n",
    "# Merge forecast with actual data for comparison\n",
    "hourly_actual_prophet = hourly_footfall_prophet.set_index('ds').reindex(forecast_prophet_df['Dt']).reset_index()\n",
    "hourly_actual_prophet.columns = ['Dt', 'Actual']\n",
    "merged_df = pd.merge(forecast_prophet_df, hourly_actual_prophet, on='Dt', how='left')\n",
    "\n",
    "# Plot actual vs forecast\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(merged_df['Dt'], merged_df['_id'], label='Forecast', color='blue')\n",
    "plt.plot(merged_df['Dt'], merged_df['Actual'], label='Actual', color='orange')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Footfall')\n",
    "plt.title('Forecast vs Actual')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot forecast components\n",
    "prophet_model.plot_components(forecast_prophet)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4def3f14-adbe-456b-9c7c-16a5fce7f080",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86257c8a-2781-444c-bc4e-b8b94080c01b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588aa96d-e64b-461f-98eb-06069d514907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f590653c-5832-4544-a50e-9a1a5f4e2831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d87a40b-11f3-4491-9e91-07fb3f1ea399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605450aa-0b83-4217-adf7-68a0c93c3950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fe69be-a3f3-426f-86f8-4e7e57fd4b15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ca2f3c-335d-4f15-b9e0-025e14a88805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ebebdd-7d9b-49f6-a9bd-38fc42368679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dd1d42-3acd-49e2-b79e-0196f70d392c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfa6fab-c2be-452b-b06c-60ba326c15d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f69bc76-3897-431b-94fa-62398beb0792",
   "metadata": {},
   "source": [
    "whole old model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ca9a44-7652-44f3-85f9-c53002a1c407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from pmdarima import auto_arima\n",
    "from prophet import Prophet\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load Dataset\n",
    "df = pd.read_csv('transaction.csv')  # Replace with your dataset path\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Manually define holidays and festivals\n",
    "holidays = pd.to_datetime(['2024-01-26', '2024-08-15', '2024-10-02'])  # Replace with real holiday dates\n",
    "festivals = pd.to_datetime(['2024-03-29', '2024-11-04', '2024-12-25'])  # Replace with real festival dates\n",
    "\n",
    "# Function to aggregate data based on frequency\n",
    "def aggregate_data(df, freq):\n",
    "    return df.resample(freq).count()\n",
    "\n",
    "# ARIMA Forecast Function\n",
    "def arima_forecast(data, periods):\n",
    "    try:\n",
    "        model = auto_arima(data, seasonal=False, stepwise=True)\n",
    "        arima_model = ARIMA(data, order=model.order)\n",
    "        arima_model_fit = arima_model.fit()\n",
    "        forecast = arima_model_fit.forecast(steps=periods)\n",
    "        forecast_df = pd.DataFrame({'timestamp': pd.date_range(start=data.index[-1], periods=periods + 1, freq='D')[1:], \n",
    "                                    'ARIMA_Forecast': forecast})\n",
    "        return forecast_df\n",
    "    except Exception as e:\n",
    "        print(f\"ARIMA Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Prophet Forecast Function\n",
    "def prophet_forecast(data, periods):\n",
    "    try:\n",
    "        data_prophet = data.reset_index()\n",
    "        data_prophet.columns = ['ds', 'y']\n",
    "        model = Prophet()\n",
    "        model.fit(data_prophet)\n",
    "        future = model.make_future_dataframe(periods=periods)\n",
    "        forecast = model.predict(future)\n",
    "        forecast_df = forecast[['ds', 'yhat']].tail(periods)\n",
    "        forecast_df.columns = ['timestamp', 'Prophet_Forecast']\n",
    "        return forecast_df\n",
    "    except Exception as e:\n",
    "        print(f\"Prophet Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Functions for hourly, daily, weekly, monthly, yearly, festival, and holiday\n",
    "def forecast_hourly(df, periods):\n",
    "    data = aggregate_data(df, 'H')\n",
    "    return forecast_combined(data, periods)\n",
    "\n",
    "def forecast_daily(df, periods):\n",
    "    data = aggregate_data(df, 'D')\n",
    "    return forecast_combined(data, periods)\n",
    "\n",
    "def forecast_weekly(df, periods):\n",
    "    data = aggregate_data(df, 'W')\n",
    "    return forecast_combined(data, periods)\n",
    "\n",
    "def forecast_monthly(df, periods, month_of_year):\n",
    "    data = df[df.index.month == month_of_year]\n",
    "    return forecast_combined(data, periods)\n",
    "\n",
    "def forecast_yearly(df, periods):\n",
    "    data = aggregate_data(df, 'Y')\n",
    "    return forecast_combined(data, periods)\n",
    "\n",
    "def forecast_festival(df, periods):\n",
    "    data = df[df.index.isin(festivals)]\n",
    "    return forecast_combined(data, periods)\n",
    "\n",
    "def forecast_holiday(df, periods):\n",
    "    data = df[df.index.isin(holidays)]\n",
    "    return forecast_combined(data, periods)\n",
    "\n",
    "# Helper function to combine ARIMA and Prophet forecasts\n",
    "def forecast_combined(data, periods):\n",
    "    arima_forecast_df = arima_forecast(data['transaction_id'], periods)\n",
    "    prophet_forecast_df = prophet_forecast(data['transaction_id'], periods)\n",
    "\n",
    "    if arima_forecast_df is not None and prophet_forecast_df is not None:\n",
    "        combined_forecast_df = pd.merge(arima_forecast_df, prophet_forecast_df, on='timestamp', how='outer')\n",
    "    else:\n",
    "        combined_forecast_df = None\n",
    "    \n",
    "    return combined_forecast_df\n",
    "\n",
    "# Example usage for weekly forecast\n",
    "forecast_type = 'monthly'  # 'hourly', 'daily', 'weekly', 'monthly', 'yearly', 'festival', 'holiday'\n",
    "periods = 365  # Number of periods to forecast\n",
    "month_of_year = 3  # Example: for forecasting a specific month (March)\n",
    "\n",
    "if forecast_type == 'hourly':\n",
    "    forecast = forecast_hourly(df, periods)\n",
    "elif forecast_type == 'daily':\n",
    "    forecast = forecast_daily(df, periods)\n",
    "elif forecast_type == 'weekly':\n",
    "    forecast = forecast_weekly(df, periods)\n",
    "elif forecast_type == 'monthly':\n",
    "    forecast = forecast_monthly(df, periods, month_of_year)\n",
    "elif forecast_type == 'yearly':\n",
    "    forecast = forecast_yearly(df, periods)\n",
    "elif forecast_type == 'festival':\n",
    "    forecast = forecast_festival(df, periods)\n",
    "elif forecast_type == 'holiday':\n",
    "    forecast = forecast_holiday(df, periods)\n",
    "\n",
    "# Save the forecasted data to CSV\n",
    "output_dir = r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_dir, f'{forecast_type}_forecast.csv')\n",
    "\n",
    "if forecast is not None:\n",
    "    forecast.to_csv(output_path, index=False)\n",
    "    print(f\"Forecast saved at: {output_path}\")\n",
    "else:\n",
    "    print(\"Forecasting failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48f37df-683b-494d-94a0-810d022d9709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdc5a0c-30a9-467f-87f1-810330ae9932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a00fc5b-9c6e-4b55-b336-894fc8c85214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from pmdarima import auto_arima\n",
    "from prophet import Prophet\n",
    "import numpy as np\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Load Dataset\n",
    "df = pd.read_csv('transaction.csv')  # Update path to the uploaded file\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Manually define holidays and festivals\n",
    "holidays = pd.to_datetime(['2024-01-26', '2024-08-15', '2024-10-02'])  # Replace with real holiday dates\n",
    "festivals = pd.to_datetime(['2024-03-29', '2024-11-04', '2024-12-25'])  # Replace with real festival dates\n",
    "\n",
    "# Function to aggregate data based on frequency\n",
    "def aggregate_data(df, freq):\n",
    "    return df.resample(freq).sum()\n",
    "\n",
    "# ARIMA Forecast Function\n",
    "def arima_forecast(data, periods):\n",
    "    if data.empty:\n",
    "        print(\"ARIMA Error: No data available for ARIMA model.\")\n",
    "        return None\n",
    "    try:\n",
    "        data_series = data.squeeze()  # Ensure data is a Series\n",
    "        model = auto_arima(data_series, seasonal=False, stepwise=True)\n",
    "        arima_model = ARIMA(data_series, order=model.order)\n",
    "        arima_model_fit = arima_model.fit()\n",
    "        forecast = arima_model_fit.forecast(steps=periods)\n",
    "        forecast_df = pd.DataFrame({\n",
    "            'timestamp': pd.date_range(start=data.index[-1] + pd.DateOffset(days=1), periods=periods, freq='D'),\n",
    "            'ARIMA_Forecast': forecast\n",
    "        })\n",
    "        return forecast_df\n",
    "    except Exception as e:\n",
    "        print(f\"ARIMA Error: {e}\")\n",
    "        return None\n",
    "# Prophet Forecast Function\n",
    "def prophet_forecast(data, periods):\n",
    "    if data.empty or data.dropna().shape[0] < 2:\n",
    "        print(\"Prophet Error: Not enough data available for Prophet model.\")\n",
    "        return None\n",
    "    try:\n",
    "        data_prophet = data.reset_index()\n",
    "        data_prophet.columns = ['ds', 'y']\n",
    "        model = Prophet()\n",
    "        model.fit(data_prophet)\n",
    "        future = model.make_future_dataframe(periods=periods)\n",
    "        forecast = model.predict(future)\n",
    "        forecast_df = forecast[['ds', 'yhat']].tail(periods)\n",
    "        forecast_df.columns = ['timestamp', 'Prophet_Forecast']\n",
    "        return forecast_df\n",
    "    except Exception as e:\n",
    "        print(f\"Prophet Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Helper function to combine ARIMA and Prophet forecasts\n",
    "def forecast_combined(data, periods):\n",
    "    if 'transaction_id' not in data.columns:\n",
    "        print(\"Error: 'transaction_id' column is missing in the data.\")\n",
    "        return None\n",
    "    \n",
    "    arima_forecast_df = arima_forecast(data['transaction_id'], periods)\n",
    "    prophet_forecast_df = prophet_forecast(data['transaction_id'], periods)\n",
    "\n",
    "    if arima_forecast_df is not None and prophet_forecast_df is not None:\n",
    "        combined_forecast_df = pd.merge(arima_forecast_df, prophet_forecast_df, on='timestamp', how='outer')\n",
    "        return combined_forecast_df\n",
    "    return None\n",
    "\n",
    "# Forecast functions with date range filtering\n",
    "def forecast_data(df, freq, start_date, end_date, periods):\n",
    "    data = df.loc[start_date:end_date]\n",
    "    if data.empty:\n",
    "        print(f\"No data available for the specified {freq} range.\")\n",
    "        return None\n",
    "    data = aggregate_data(data, freq)\n",
    "    return forecast_combined(data, periods)\n",
    "\n",
    "def forecast_hourly_by_hour(df, periods):\n",
    "    def process_hour(hour):\n",
    "        hour_data = df[df.index.hour == hour]\n",
    "        if hour_data.empty:\n",
    "            print(f\"No data available for the {hour+1}th hour.\")\n",
    "            return None\n",
    "        hour_data = aggregate_data(hour_data, 'D')  # Aggregate by day\n",
    "        forecast = forecast_combined(hour_data, periods)\n",
    "        if forecast is not None:\n",
    "            forecast['hour'] = hour + 1  # Add the hour identifier\n",
    "            return forecast\n",
    "        return None\n",
    "\n",
    "    forecasts = Parallel(n_jobs=-1)(delayed(process_hour)(hour) for hour in range(24))\n",
    "    forecast_df = pd.concat([f for f in forecasts if f is not None], ignore_index=True)\n",
    "    return forecast_df\n",
    "\n",
    "# Example usage for forecast with different types\n",
    "forecast_type = 'hourly_by_hour'  # 'hourly', 'daily', 'weekly', 'monthly', 'yearly', 'festival', 'holiday', 'hourly_by_hour'\n",
    "start_date = '2024-01-01'  # Example start date\n",
    "end_date = '2024-12-31'    # Example end date\n",
    "periods = 30  # Number of periods to forecast\n",
    "\n",
    "forecast_functions = {\n",
    "    'hourly': lambda: forecast_data(df, 'H', start_date, end_date, periods),\n",
    "    'daily': lambda: forecast_data(df, 'D', start_date, end_date, periods),\n",
    "    'weekly': lambda: forecast_data(df, 'W', start_date, end_date, periods),\n",
    "    'monthly': lambda: forecast_data(df, 'M', start_date, end_date, periods),\n",
    "    'yearly': lambda: forecast_data(df, 'Y', start_date, end_date, periods),\n",
    "    'festival': lambda: forecast_combined(df[df.index.isin(festivals)], periods),\n",
    "    'holiday': lambda: forecast_combined(df[df.index.isin(holidays)], periods),\n",
    "    'hourly_by_hour': lambda: forecast_hourly_by_hour(df, periods)\n",
    "}\n",
    "\n",
    "if forecast_type in forecast_functions:\n",
    "    forecast = forecast_functions[forecast_type]()\n",
    "else:\n",
    "    print(f\"Unknown forecast type: {forecast_type}\")\n",
    "    forecast = None\n",
    "\n",
    "# Save the forecasted data to CSV\n",
    "output_dir = r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_dir, f'{forecast_type}_forecast.csv')\n",
    "\n",
    "if forecast is not None:\n",
    "    forecast.to_csv(output_path, index=False)\n",
    "    print(f\"Forecast saved at: {output_path}\")\n",
    "else:\n",
    "    print(\"Forecasting failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72003bae-c1e0-41cf-9b8b-900d65efb448",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b70ee7-dd75-44a0-ab71-d7f56c56bd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from pmdarima import auto_arima\n",
    "from prophet import Prophet\n",
    "import os\n",
    "\n",
    "# Load Dataset\n",
    "df = pd.read_csv('transaction_updated.csv')\n",
    "\n",
    "# Data Preprocessing\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], format='%Y-%m-%dT%H-%M-%S.%fZ', errors='coerce')\n",
    "df = df.dropna(subset=['timestamp'])\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Filtering based on user input for stn no and EqN\n",
    "stn_no = input(\"Enter station number (or 'all' for all stations): \")\n",
    "eqn_no = input(\"Enter equipment number (or 'all' for all equipment): \")\n",
    "\n",
    "if stn_no.lower() != 'all':\n",
    "    df = df[df['stn_no'] == int(stn_no)]\n",
    "if eqn_no.lower() != 'all':\n",
    "    df = df[df['EqN'] == int(eqn_no)]\n",
    "\n",
    "# Aggregating the Footfall at Hourly Level\n",
    "hourly_footfall = df.resample('H').count().asfreq('H')\n",
    "\n",
    "# Time-Series Decomposition\n",
    "decomposition = seasonal_decompose(hourly_footfall['transaction_id'], model='additive')\n",
    "fig = decomposition.plot()\n",
    "fig.suptitle('Time-Series Decomposition: Trend, Seasonal, and Residuals', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Specify Forecast Period\n",
    "start_date = pd.to_datetime(input(\"Enter forecast start date (YYYY-MM-DD): \"))\n",
    "end_date = pd.to_datetime(input(\"Enter forecast end date (YYYY-MM-DD): \"))\n",
    "forecast_period = (end_date - start_date).days * 24  # Forecast period in hours\n",
    "\n",
    "# File Path Base\n",
    "base_path = r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors'\n",
    "\n",
    "def save_forecast_data(df, model_name, file_suffixes):\n",
    "    for suffix, freq in file_suffixes:\n",
    "        aggregated_df = df.resample(freq).sum()\n",
    "        if freq == 'W':\n",
    "            aggregated_df['week_of_year'] = aggregated_df.index.isocalendar().week\n",
    "            aggregated_df['day_of_week'] = aggregated_df.index.day_name()\n",
    "        elif freq == 'M':\n",
    "            aggregated_df['month_name'] = aggregated_df.index.month_name()\n",
    "        elif freq in ['D', 'H']:\n",
    "            aggregated_df['day_of_week'] = aggregated_df.index.day_name()\n",
    "        aggregated_df.to_csv(os.path.join(base_path, f'{freq}_forecast_{model_name}.csv'))\n",
    "\n",
    "    # Aggregation by Hour of Day\n",
    "    hourly_aggr = df.groupby(df.index.hour).sum().reset_index()\n",
    "    hourly_aggr.to_csv(os.path.join(base_path, f'hourly_aggr_{model_name}.csv'), index=False)\n",
    "\n",
    "# ARIMA Forecasting\n",
    "try:\n",
    "    # Fit ARIMA model and forecast\n",
    "    auto_model = auto_arima(hourly_footfall['transaction_id'].dropna(), seasonal=False, stepwise=True)\n",
    "    arima_model = ARIMA(hourly_footfall['transaction_id'], order=auto_model.order).fit()\n",
    "    arima_forecast = arima_model.forecast(steps=forecast_period)\n",
    "    arima_forecast_df = pd.DataFrame({'timestamp': pd.date_range(hourly_footfall.index[-1] + pd.Timedelta(hours=1), periods=len(arima_forecast), freq='H'), 'ARIMA_Forecast': arima_forecast})\n",
    "    arima_forecast_df = arima_forecast_df[arima_forecast_df['timestamp'] >= start_date]\n",
    "    arima_forecast_df['ARIMA_Forecast'] = arima_forecast_df['ARIMA_Forecast'].round()\n",
    "\n",
    "    # Save ARIMA Forecast Data\n",
    "    arima_forecast_df.to_csv(os.path.join(base_path, f'arima_forecast_hourly_{stn_no}_{eqn_no}.csv'), index=False)\n",
    "    save_forecast_data(arima_forecast_df.set_index('timestamp'), 'arima', [('W', 'W'), ('M', 'M'), ('Y', 'Y'), ('D', 'D')])\n",
    "\n",
    "    # Plot ARIMA Forecast\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(hourly_footfall.index, hourly_footfall['transaction_id'], label='Observed', color='blue')\n",
    "    plt.plot(arima_forecast_df['timestamp'], arima_forecast_df['ARIMA_Forecast'], label='ARIMA Forecast', color='orange')\n",
    "    plt.title(f'ARIMA Model: Observed vs Forecasted Footfall ({start_date.date()} to {end_date.date()})')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Footfall')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred with ARIMA model: {e}\")\n",
    "\n",
    "# Prophet Model - Forecasting\n",
    "hourly_footfall_prophet = hourly_footfall.reset_index()[['timestamp', 'transaction_id']]\n",
    "hourly_footfall_prophet.columns = ['ds', 'y']\n",
    "hourly_footfall_prophet['ds'] = pd.to_datetime(hourly_footfall_prophet['ds'])\n",
    "hourly_footfall_prophet['y'] = pd.to_numeric(hourly_footfall_prophet['y'], errors='coerce')\n",
    "\n",
    "prophet_model = Prophet()\n",
    "prophet_model.fit(hourly_footfall_prophet)\n",
    "\n",
    "future_prophet = pd.DataFrame({'ds': pd.date_range(start=start_date, end=end_date, freq='H')})\n",
    "forecast_prophet = prophet_model.predict(future_prophet)\n",
    "prophet_forecast_df = forecast_prophet[['ds', 'yhat']].rename(columns={'ds': 'timestamp', 'yhat': 'transaction_id'})\n",
    "prophet_forecast_df['transaction_id'] = prophet_forecast_df['transaction_id'].round()\n",
    "\n",
    "# Save Prophet Forecast Data\n",
    "prophet_forecast_df.to_csv(os.path.join(base_path, f'hourly_footfall_forecast_prophet_{stn_no}_{eqn_no}.csv'), index=False)\n",
    "save_forecast_data(prophet_forecast_df.set_index('timestamp'), 'prophet', [('W', 'W'), ('M', 'M'), ('Y', 'Y'), ('D', 'D')])\n",
    "\n",
    "# Display the Prophet Forecast DataFrame\n",
    "print(prophet_forecast_df.head())\n",
    "print(\"Files saved to their respective destinations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24aa67f-b353-4cb7-81de-abd29d37b7e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41cc249-c00b-40ed-878c-c27fad2e7917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from prophet import Prophet\n",
    "import os\n",
    "\n",
    "# Load Dataset\n",
    "df = pd.read_csv('transaction_updated.csv')  # Replace with your dataset path\n",
    "\n",
    "# Data Preprocessing\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], format='%Y-%m-%dT%H-%M-%S.%fZ', errors='coerce')\n",
    "\n",
    "# Check for any rows where parsing failed\n",
    "invalid_dates = df[df['timestamp'].isnull()]\n",
    "if not invalid_dates.empty:\n",
    "    print(\"Some dates could not be parsed:\")\n",
    "    print(invalid_dates)\n",
    "\n",
    "# Drop rows with invalid dates if needed\n",
    "df = df.dropna(subset=['timestamp'])\n",
    "\n",
    "# Continue with setting the index\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Filtering based on user input for stn no and EqN\n",
    "stn_no = input(\"Enter station number (or 'all' for all stations): \")\n",
    "eqn_no = input(\"Enter equipment number (or 'all' for all equipment): \")\n",
    "\n",
    "# Apply filters\n",
    "if stn_no.lower() != 'all':\n",
    "    df = df[df['stn_no'] == int(stn_no)]\n",
    "if eqn_no.lower() != 'all':\n",
    "    df = df[df['EqN'] == int(eqn_no)]\n",
    "\n",
    "# Aggregating the Footfall at Hourly Level\n",
    "hourly_footfall = df.resample('H').count()  # Hourly footfall\n",
    "\n",
    "# Set frequency for hourly_footfall\n",
    "hourly_footfall = hourly_footfall.asfreq('H')\n",
    "\n",
    "# Time-Series Decomposition\n",
    "decomposition = seasonal_decompose(hourly_footfall['transaction_id'], model='additive')\n",
    "fig = decomposition.plot()\n",
    "fig.suptitle('Time-Series Decomposition: Trend, Seasonal, and Residuals', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Specify Forecast Period\n",
    "start_date = pd.to_datetime(input(\"Enter forecast start date (YYYY-MM-DD): \"))\n",
    "end_date = pd.to_datetime(input(\"Enter forecast end date (YYYY-MM-DD): \"))\n",
    "forecast_period = (end_date - start_date).days * 24  # Forecast period in hours\n",
    "\n",
    "# File Path Base\n",
    "base_path = r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors'\n",
    "\n",
    "# Prophet Model - Alternative Time-Series Forecasting\n",
    "hourly_footfall_prophet = hourly_footfall.reset_index()\n",
    "\n",
    "# Ensure the DataFrame only has 'timestamp' and 'transaction_id' after resetting the index\n",
    "hourly_footfall_prophet = hourly_footfall_prophet[['timestamp', 'transaction_id']]\n",
    "hourly_footfall_prophet.columns = ['ds', 'y']  # Rename columns for Prophet\n",
    "\n",
    "# Convert 'ds' to datetime and 'y' to numeric\n",
    "hourly_footfall_prophet['ds'] = pd.to_datetime(hourly_footfall_prophet['ds'])\n",
    "hourly_footfall_prophet['y'] = pd.to_numeric(hourly_footfall_prophet['y'], errors='coerce')\n",
    "\n",
    "# Fit Prophet model\n",
    "prophet_model = Prophet()\n",
    "prophet_model.fit(hourly_footfall_prophet)\n",
    "\n",
    "# Create future dataframe starting from the user-specified start_date\n",
    "future_df = pd.date_range(start=start_date, end=end_date, freq='H')\n",
    "future_prophet = pd.DataFrame({'ds': future_df})\n",
    "forecast_prophet = prophet_model.predict(future_prophet)\n",
    "\n",
    "# Creating a DataFrame for Prophet Forecast\n",
    "prophet_forecast_df = forecast_prophet[['ds', 'yhat']]\n",
    "prophet_forecast_df.columns = ['timestamp', 'transaction_id']\n",
    "\n",
    "# Round off the Prophet forecasted data\n",
    "prophet_forecast_df['transaction_id'] = prophet_forecast_df['transaction_id'].round()\n",
    "\n",
    "# Save Prophet Forecast to CSV\n",
    "prophet_forecast_df.to_csv(os.path.join(base_path, f'hourly_footfall_forecast_prophet_{stn_no}_{eqn_no}.csv'), index=False)\n",
    "\n",
    "# Aggregation of Prophet Forecast\n",
    "daily_forecast_prophet = prophet_forecast_df.copy()\n",
    "daily_forecast_prophet.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Aggregation: Daily, Weekly, Monthly, Yearly\n",
    "daily_forecast_prophet_resampled = daily_forecast_prophet.resample('D').sum()\n",
    "weekly_forecast_prophet_resampled = daily_forecast_prophet.resample('W').sum()\n",
    "monthly_forecast_prophet_resampled = daily_forecast_prophet.resample('M').sum()\n",
    "yearly_forecast_prophet_resampled = daily_forecast_prophet.resample('Y').sum()\n",
    "\n",
    "# Weekend and Weekday Aggregation\n",
    "weekend_forecast_prophet_resampled = daily_forecast_prophet[daily_forecast_prophet.index.dayofweek >= 5].resample('D').sum()\n",
    "weekday_forecast_prophet_resampled = daily_forecast_prophet[daily_forecast_prophet.index.dayofweek < 5].resample('D').sum()\n",
    "\n",
    "# Add week of year and day of week to weekly forecast\n",
    "weekly_forecast_prophet_resampled['week_of_year'] = weekly_forecast_prophet_resampled.index.isocalendar().week\n",
    "weekly_forecast_prophet_resampled['day_of_week'] = weekly_forecast_prophet_resampled.index.day_name()\n",
    "\n",
    "# Add month name to monthly forecast\n",
    "monthly_forecast_prophet_resampled['month_name'] = monthly_forecast_prophet_resampled.index.month_name()\n",
    "\n",
    "# Add day of week to weekend and weekday forecasts\n",
    "weekend_forecast_prophet_resampled['day_of_week'] = weekend_forecast_prophet_resampled.index.day_name()\n",
    "weekday_forecast_prophet_resampled['day_of_week'] = weekday_forecast_prophet_resampled.index.day_name()\n",
    "\n",
    "# Save Aggregated Forecast Data to CSV\n",
    "daily_forecast_prophet_resampled.to_csv(os.path.join(base_path, f'daily_forecast_prophet_{stn_no}_{eqn_no}.csv'))\n",
    "weekly_forecast_prophet_resampled.to_csv(os.path.join(base_path, f'weekly_forecast_prophet_{stn_no}_{eqn_no}.csv'))\n",
    "monthly_forecast_prophet_resampled.to_csv(os.path.join(base_path, f'monthly_forecast_prophet_{stn_no}_{eqn_no}.csv'))\n",
    "yearly_forecast_prophet_resampled.to_csv(os.path.join(base_path, f'yearly_forecast_prophet_{stn_no}_{eqn_no}.csv'))\n",
    "weekend_forecast_prophet_resampled.to_csv(os.path.join(base_path, f'weekend_forecast_prophet_{stn_no}_{eqn_no}.csv'))\n",
    "weekday_forecast_prophet_resampled.to_csv(os.path.join(base_path, f'weekday_forecast_prophet_{stn_no}_{eqn_no}.csv'))\n",
    "\n",
    "# Aggregation of Prophet Forecast Data by Hour of Day\n",
    "prophet_forecast_df['hour'] = prophet_forecast_df['timestamp'].dt.hour\n",
    "hourly_aggr_prophet = prophet_forecast_df.groupby('hour')['transaction_id'].sum().reset_index()\n",
    "\n",
    "# Save Aggregated Hourly Prophet Forecast Data to CSV\n",
    "hourly_aggr_prophet.to_csv(os.path.join(base_path, f'hourly_aggr_prophet_forecast_{stn_no}_{eqn_no}.csv'), index=False)\n",
    "\n",
    "# Display the Prophet Forecast DataFrame\n",
    "print(prophet_forecast_df.head())\n",
    "\n",
    "print(\"Files saved to their respective destinations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd058abc-cbcf-4add-b49a-2e98f1bc5e07",
   "metadata": {},
   "outputs": [],
   "source": [
    ".index, hourly_df['transaction_id'], label='Hourly Footfall Forecast', color='orange')\n",
    "        plt.title('Hourly Aggregated Forecast Footfall')\n",
    "        plt.xlabel('Hour')\n",
    "        plt.ylabel('Transaction ID')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    \n",
    "    else:\n",
    "        # Load Dataset\n",
    "        file_path = file_paths.get(data_type)\n",
    "        if file_path:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Data Preprocessing\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df.set_index('timestamp', inplace=True)\n",
    "            \n",
    "            # Print the raw data to check its structure\n",
    "            print(f\"Raw data for {data_type}:\")\n",
    "            print(df.head())\n",
    "\n",
    "            # Plotting the raw data\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.plot(df.index, df['transaction_id'], label=f'Raw {data_type.capitalize()} Footfall', color='blue')\n",
    "            plt.title(f'Raw {data_type.capitalize()} Footfall Over Time')\n",
    "            plt.xlabel('Timestamp')\n",
    "            plt.ylabel('Transaction ID')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"File path for '{data_type}' data type is not provided.\")\n",
    "\n",
    "def main():\n",
    "    # Define the base path for output files\n",
    "    base_path = r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors'\n",
    "    \n",
    "    # Get station number and equation number from the user\n",
    "    stn_no = input(\"Enter the station number (stn_no): \").strip()\n",
    "    eqn_no = input(\"Enter the equation number (eqn_no): \").strip()\n",
    "    \n",
    "    # Define file paths for different forecast types\n",
    "    file_paths = {\n",
    "        'daily': os.path.join(base_path, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be74c85-5620-441e-a611-9f0be71e5983",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c340f74b-f72e-4613-9bd4-12830538e6e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846dd186-6e2e-45bc-82ab-64bdf58bc4c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eedc82a-f81d-4878-ae3a-690e76bab19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from prophet import Prophet\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "\n",
    "# Load Dataset\n",
    "try:\n",
    "    df = pd.read_csv('transaction.csv')  # Replace with your dataset path\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: The file was not found.\")\n",
    "    exit()\n",
    "\n",
    "# Data Preprocessing\n",
    "df['Dt'] = pd.to_datetime(df['Dt'], errors='coerce')\n",
    "invalid_dates = df[df['Dt'].isnull()]\n",
    "if not invalid_dates.empty:\n",
    "    print(\"Some dates could not be parsed:\")\n",
    "    print(invalid_dates)\n",
    "df = df.dropna(subset=['Dt'])\n",
    "df.set_index('Dt', inplace=True)\n",
    "\n",
    "# Filtering based on user input\n",
    "stn_no = input(\"Enter station number (or 'all' for all stations): \")\n",
    "eqn_no = input(\"Enter equipment number (or 'all' for all equipment): \")\n",
    "if stn_no.lower() != 'all':\n",
    "    df = df[df['Sta'] == int(stn_no)]\n",
    "if eqn_no.lower() != 'all':\n",
    "    df = df[df['EqN'] == int(eqn_no)]\n",
    "\n",
    "# Aggregating the Footfall at Hourly Level\n",
    "hourly_footfall = df.resample('H').count()\n",
    "hourly_footfall = hourly_footfall.asfreq('H')\n",
    "\n",
    "# Time-Series Decomposition for visual inspection\n",
    "decomposition = seasonal_decompose(hourly_footfall['_id'], model='additive')\n",
    "fig = decomposition.plot()\n",
    "fig.suptitle('Time-Series Decomposition: Trend, Seasonal, and Residuals', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Specify Forecast Period\n",
    "while True:\n",
    "    try:\n",
    "        start_date = pd.to_datetime(input(\"Enter forecast start date (YYYY-MM-DD): \"))\n",
    "        end_date = pd.to_datetime(input(\"Enter forecast end date (YYYY-MM-DD): \"))\n",
    "        if start_date >= end_date:\n",
    "            raise ValueError(\"Start date must be before end date.\")\n",
    "        break\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}. Please enter valid dates.\")\n",
    "\n",
    "forecast_period = (end_date - start_date).days * 24\n",
    "\n",
    "# File Path Base\n",
    "base_path = r'C:\\Users\\admin\\Desktop\\airline\\sensor-file-ridership\\output of sensors'\n",
    "\n",
    "# Convert holidays to DataFrame\n",
    "holiday_df = pd.DataFrame({\n",
    "    'holiday': 'Indian Holiday',\n",
    "    'ds': list(pd.to_datetime(list(indian_holidays.keys()))),\n",
    "    'lower_window': 0,\n",
    "    'upper_window': 1,\n",
    "})\n",
    "\n",
    "# Prophet Model - Time-Series Forecasting with Seasonalities\n",
    "hourly_footfall_prophet = hourly_footfall.reset_index()\n",
    "hourly_footfall_prophet = hourly_footfall_prophet[['Dt', '_id']]\n",
    "hourly_footfall_prophet.columns = ['ds', 'y']\n",
    "hourly_footfall_prophet['ds'] = hourly_footfall_prophet['ds'].dt.tz_localize(None)\n",
    "hourly_footfall_prophet['y'] = pd.to_numeric(hourly_footfall_prophet['y'], errors='coerce')\n",
    "\n",
    "prophet_model = Prophet(\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    daily_seasonality=True,\n",
    "    seasonality_prior_scale=20,  # Increased flexibility\n",
    "    holidays_prior_scale=20,    # Increased impact of holidays\n",
    "    changepoint_prior_scale=0.2  # Increased flexibility in trend changes\n",
    ")\n",
    "\n",
    "prophet_model.add_country_holidays(country_name='US')  # Example of adding country holidays\n",
    "\n",
    "prophet_model.fit(hourly_footfall_prophet)\n",
    "\n",
    "# Create future dataframe\n",
    "future_df = pd.date_range(start=start_date, end=end_date, freq='H')\n",
    "future_prophet = pd.DataFrame({'ds': future_df})\n",
    "\n",
    "# Forecast with Prophet model\n",
    "forecast_prophet = prophet_model.predict(future_prophet)\n",
    "\n",
    "# Creating DataFrame for Prophet Forecast\n",
    "prophet_forecast_df = forecast_prophet[['ds', 'yhat']]\n",
    "prophet_forecast_df.columns = ['Dt', '_id']\n",
    "prophet_forecast_df['_id'] = prophet_forecast_df['_id'].round()\n",
    "\n",
    "# Save Prophet Forecast to CSV\n",
    "prophet_forecast_df.to_csv(os.path.join(base_path, f'hourly_footfall_forecast_prophet_{stn_no}_{eqn_no}.csv'), index=False)\n",
    "\n",
    "# Aggregation of Prophet Forecast\n",
    "daily_forecast_prophet = prophet_forecast_df.copy()\n",
    "daily_forecast_prophet.set_index('Dt', inplace=True)\n",
    "\n",
    "# Aggregation: Daily, Weekly, Monthly, Yearly\n",
    "daily_forecast_prophet_resampled = daily_forecast_prophet.resample('D').sum()\n",
    "weekly_forecast_prophet_resampled = daily_forecast_prophet.resample('W').sum()\n",
    "monthly_forecast_prophet_resampled = daily_forecast_prophet.resample('M').sum()\n",
    "yearly_forecast_prophet_resampled = daily_forecast_prophet.resample('Y').sum()\n",
    "\n",
    "# Weekend and Weekday Aggregation\n",
    "weekend_forecast_prophet_resampled = daily_forecast_prophet[daily_forecast_prophet.index.dayofweek >= 5].resample('D').sum()\n",
    "weekday_forecast_prophet_resampled = daily_forecast_prophet[daily_forecast_prophet.index.dayofweek < 5].resample('D').sum()\n",
    "\n",
    "# Add week of year and day of week to weekly forecast\n",
    "weekly_forecast_prophet_resampled['week_of_year'] = weekly_forecast_prophet_resampled.index.isocalendar().week\n",
    "weekly_forecast_prophet_resampled['day_of_week'] = weekly_forecast_prophet_resampled.index.day_name()\n",
    "\n",
    "# Add month name to monthly forecast\n",
    "monthly_forecast_prophet_resampled['month_name'] = monthly_forecast_prophet_resampled.index.month_name()\n",
    "\n",
    "# Add day of week to weekend and weekday forecasts\n",
    "weekend_forecast_prophet_resampled['day_of_week'] = weekend_forecast_prophet_resampled.index.day_name()\n",
    "weekday_forecast_prophet_resampled['day_of_week'] = weekday_forecast_prophet_resampled.index.day_name()\n",
    "\n",
    "# Save Aggregated Forecast Data to CSV\n",
    "daily_forecast_prophet_resampled.to_csv(os.path.join(base_path, f'daily_forecast_prophet_{stn_no}_{eqn_no}.csv'))\n",
    "weekly_forecast_prophet_resampled.to_csv(os.path.join(base_path, f'weekly_forecast_prophet_{stn_no}_{eqn_no}.csv'))\n",
    "monthly_forecast_prophet_resampled.to_csv(os.path.join(base_path, f'monthly_forecast_prophet_{stn_no}_{eqn_no}.csv'))\n",
    "yearly_forecast_prophet_resampled.to_csv(os.path.join(base_path, f'yearly_forecast_prophet_{stn_no}_{eqn_no}.csv'))\n",
    "weekend_forecast_prophet_resampled.to_csv(os.path.join(base_path, f'weekend_forecast_prophet_{stn_no}_{eqn_no}.csv'))\n",
    "weekday_forecast_prophet_resampled.to_csv(os.path.join(base_path, f'weekday_forecast_prophet_{stn_no}_{eqn_no}.csv'))\n",
    "\n",
    "# Aggregation of Prophet Forecast Data by Hour of Day\n",
    "prophet_forecast_df['hour'] = prophet_forecast_df['Dt'].dt.hour\n",
    "hourly_aggr_prophet = prophet_forecast_df.groupby('hour')['_id'].sum().reset_index()\n",
    "hourly_aggr_prophet.to_csv(os.path.join(base_path, f'hourly_aggr_prophet_forecast_{stn_no}_{eqn_no}.csv'), index=False)\n",
    "\n",
    "# Validate Performance for Hourly Forecast\n",
    "# Ensure date formats and time zones are aligned\n",
    "hourly_footfall_prophet['ds'] = pd.to_datetime(hourly_footfall_prophet['ds']).dt.tz_localize(None)\n",
    "prophet_forecast_df['Dt'] = pd.to_datetime(prophet_forecast_df['Dt']).dt.tz_localize(None)\n",
    "\n",
    "# Reindex and merge data\n",
    "hourly_actual_prophet = hourly_footfall_prophet.set_index('ds').reindex(prophet_forecast_df['Dt']).reset_index()\n",
    "hourly_actual_prophet.columns = ['Dt', 'Actual']\n",
    "\n",
    "# Print date ranges and samples for debugging\n",
    "print(\"Hourly Footfall Date Range:\", hourly_footfall_prophet['ds'].min(), \"to\", hourly_footfall_prophet['ds'].max())\n",
    "print(\"Forecast Date Range:\", prophet_forecast_df['Dt'].min(), \"to\", prophet_forecast_df['Dt'].max())\n",
    "print(\"Sample dates from hourly_footfall_prophet:\")\n",
    "print(hourly_footfall_prophet.head())\n",
    "print(\"Sample dates from prophet_forecast_df:\")\n",
    "print(prophet_forecast_df.head())\n",
    "\n",
    "# Check if the hourly_actual_prophet DataFrame has actual data\n",
    "if hourly_actual_prophet['Actual'].isnull().all():\n",
    "    print(\"Warning: No actual data available for the forecast period.\")\n",
    "else:\n",
    "    print(\"Actual data available for some or all forecast periods.\")\n",
    "\n",
    "# Merge forecast and actual data\n",
    "merged_hourly_df = pd.merge(prophet_forecast_df, hourly_actual_prophet, on='Dt', how='inner')\n",
    "\n",
    "# Plotting Forecast vs Actual\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(merged_hourly_df['Dt'], merged_hourly_df['_id'], label='Forecast', color='blue')\n",
    "plt.plot(merged_hourly_df['Dt'], merged_hourly_df['Actual'], label='Actual', color='orange')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Footfall')\n",
    "plt.title('Hourly Forecast vs Actual')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "prophet_model.plot_components(forecast_prophet)\n",
    "plt.show()\n",
    "\n",
    "# Error Metrics\n",
    "mae = mean_absolute_error(merged_hourly_df['Actual'], merged_hourly_df['_id'])\n",
    "mse = mean_squared_error(merged_hourly_df['Actual'], merged_hourly_df['_id'])\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"MAE: {mae}, MSE: {mse}, RMSE: {rmse}\")\n",
    "\n",
    "# Cross-validation for Prophet Model\n",
    "cv_results = cross_validation(prophet_model, initial='365 days', period='180 days', horizon='30 days', parallel=\"processes\")\n",
    "cv_metrics = performance_metrics(cv_results)\n",
    "print(cv_metrics)\n",
    "\n",
    "# Residual Analysis\n",
    "merged_hourly_df['Residual'] = merged_hourly_df['Actual'] - merged_hourly_df['_id']\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(merged_hourly_df['Dt'], merged_hourly_df['Residual'], label='Residuals')\n",
    "plt.axhline(y=0, color='r', linestyle='--', label='Zero Line')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Residual')\n",
    "plt.title('Residuals Analysis')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Create DataFrame for holidays\n",
    "holiday_df_actual = hourly_footfall_prophet[hourly_footfall_prophet['ds'].isin(holiday_df['ds'])]\n",
    "\n",
    "# Aggregate total footfall on holidays\n",
    "holiday_agg = holiday_df_actual.groupby('ds').agg({'y': 'sum'}).reset_index()\n",
    "\n",
    "# Plot holiday data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(holiday_agg['ds'], holiday_agg['y'], color='green')\n",
    "plt.xlabel('Holiday Date')\n",
    "plt.ylabel('Total Footfall')\n",
    "plt.title('Footfall on Holidays')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c94d69-cf5b-4df3-a66f-8368a83d7f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter station number (or 'all' for all stations):  all\n",
      "Enter equipment number (or 'all' for all equipment):  all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_6820\\1349273101.py:23: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  hourly_footfall = df.resample('H').count()\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_6820\\1349273101.py:24: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  hourly_footfall = hourly_footfall.asfreq('H')\n",
      "The provided DatetimeIndex was associated with a timezone, which is currently not supported by xarray. To avoid unexpected behaviour, the tz information was removed. Consider calling `ts.time_index.tz_localize(UTC)` when exporting the results.To plot the series with the right time steps, consider setting the matplotlib.pyplot `rcParams['timezone']` parameter to automatically convert the time axis back to the original timezone.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name            | Type             | Params | Mode \n",
      "-------------------------------------------------------------\n",
      "0 | criterion       | MSELoss          | 0      | train\n",
      "1 | train_criterion | MSELoss          | 0      | train\n",
      "2 | val_criterion   | MSELoss          | 0      | train\n",
      "3 | train_metrics   | MetricCollection | 0      | train\n",
      "4 | val_metrics     | MetricCollection | 0      | train\n",
      "5 | stacks          | ModuleList       | 6.2 M  | train\n",
      "-------------------------------------------------------------\n",
      "6.2 M     Trainable params\n",
      "1.4 K     Non-trainable params\n",
      "6.2 M     Total params\n",
      "24.796    Total estimated model params size (MB)\n",
      "396       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8770a07e2f864d63b237d01f7fcb3286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                      | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from darts import TimeSeries\n",
    "from darts.models import NBEATSModel\n",
    "from darts.metrics import mae, rmse\n",
    "\n",
    "# Load and preprocess your data\n",
    "df = pd.read_csv('transaction.csv')  # Replace with your dataset path\n",
    "df['Dt'] = pd.to_datetime(df['Dt'], errors='coerce')\n",
    "df = df.dropna(subset=['Dt'])\n",
    "df.set_index('Dt', inplace=True)\n",
    "\n",
    "# Filtering based on user input\n",
    "stn_no = input(\"Enter station number (or 'all' for all stations): \")\n",
    "eqn_no = input(\"Enter equipment number (or 'all' for all equipment): \")\n",
    "if stn_no.lower() != 'all':\n",
    "    df = df[df['Sta'] == int(stn_no)]\n",
    "if eqn_no.lower() != 'all':\n",
    "    df = df[df['EqN'] == int(eqn_no)]\n",
    "\n",
    "# Aggregating the Footfall at Hourly Level\n",
    "hourly_footfall = df.resample('H').count()\n",
    "hourly_footfall = hourly_footfall.asfreq('H')\n",
    "\n",
    "# Convert to TimeSeries\n",
    "series = TimeSeries.from_dataframe(hourly_footfall, value_cols='_id')\n",
    "\n",
    "# Split data into training and test sets\n",
    "train, test = series[:-48], series[-48:]\n",
    "\n",
    "# Initialize and train N-BEATS model\n",
    "model = NBEATSModel(input_chunk_length=24, output_chunk_length=24, n_epochs=100)\n",
    "model.fit(train, verbose=True)\n",
    "\n",
    "# Forecast\n",
    "forecast = model.predict(len(test))\n",
    "\n",
    "# Evaluate the model\n",
    "error_mae = mae(test, forecast)\n",
    "error_rmse = rmse(test, forecast)\n",
    "\n",
    "print(f'MAE: {error_mae}')\n",
    "print(f'RMSE: {error_rmse}')\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 6))\n",
    "series.plot(label='Actual')\n",
    "forecast.plot(label='Forecast', color='orange')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Footfall')\n",
    "plt.title('Hourly Forecast vs Actual')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98490f5c-5f67-40ab-8542-85d688c705ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "13ca096a0c104eec8584ebb3ce224006": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_2e90046cf3d44415b97d775f996e9021",
       "style": "IPY_MODEL_60dee7837a44429e84c140afa0c31d6d",
       "value": "Epoch 0:  13%"
      }
     },
     "29515b7862714cb8a9f388ac4ac1f102": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "2e90046cf3d44415b97d775f996e9021": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4fe71cb1a545456a9e980806bcf45451": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_aa403456b2a5481cb10f99eee9487613",
       "max": 891,
       "style": "IPY_MODEL_29515b7862714cb8a9f388ac4ac1f102",
       "value": 112
      }
     },
     "602112b445ff4169aa2382cdc7b7c739": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "100%"
      }
     },
     "60dee7837a44429e84c140afa0c31d6d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8770a07e2f864d63b237d01f7fcb3286": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_13ca096a0c104eec8584ebb3ce224006",
        "IPY_MODEL_4fe71cb1a545456a9e980806bcf45451",
        "IPY_MODEL_a4ceecc1ec734c8b806e442221506e92"
       ],
       "layout": "IPY_MODEL_602112b445ff4169aa2382cdc7b7c739"
      }
     },
     "a4ceecc1ec734c8b806e442221506e92": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_c2bde3e6a24545f1ae2356439b1e658c",
       "style": "IPY_MODEL_b208b39f898d438493ce184de043cc75",
       "value": " 112/891 [00:45&lt;05:19,  2.44it/s, train_loss=4.05e+4]"
      }
     },
     "aa403456b2a5481cb10f99eee9487613": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "b208b39f898d438493ce184de043cc75": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c2bde3e6a24545f1ae2356439b1e658c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
