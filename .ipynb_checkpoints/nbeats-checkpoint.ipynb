{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "14bf81ae-581e-4bc3-9302-ee8ebc7e0395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: torch in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.0.0+cu118)\n",
      "Collecting torch\n",
      "  Using cached torch-2.4.1-cp311-cp311-win_amd64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Using cached torch-2.4.1-cp311-cp311-win_amd64.whl (199.4 MB)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.0.0+cu118\n",
      "    Uninstalling torch-2.0.0+cu118:\n",
      "      Successfully uninstalled torch-2.0.0+cu118\n",
      "Successfully installed torch-2.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\~vfuser'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\~orch'.\n",
      "  You can safely remove it manually.\n",
      "WARNING: Ignoring invalid distribution ~otebook (C:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "darts 0.30.0 requires numpy<2.0.0,>=1.19.0, but you have numpy 2.1.1 which is incompatible.\n",
      "nbeats-pytorch 1.8.0 requires protobuf<=3.20, but you have protobuf 4.25.4 which is incompatible.\n",
      "torchaudio 2.0.0+cu118 requires torch==2.0.0, but you have torch 2.4.1 which is incompatible.\n",
      "torchvision 0.15.0+cu118 requires torch==2.0.0, but you have torch 2.4.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade numpy torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34b6523-34fc-4541-b18a-cb33ce168536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_12120\\1775746677.py:25: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  hourly_transaction = df.resample('H').size()  # Hourly transaction count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100, Loss: 0.002160428719351564\n",
      "Epoch 10/100, Loss: 0.0014632669092719912\n",
      "Epoch 20/100, Loss: 0.001320534339548439\n",
      "Epoch 30/100, Loss: 0.0012219789715103566\n",
      "Epoch 40/100, Loss: 0.0011828401763581697\n",
      "Epoch 50/100, Loss: 0.0011318870675920962\n",
      "Epoch 60/100, Loss: 0.0010924330173453334\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load Dataset\n",
    "df = pd.read_csv('AFC.3_74_objEvent.csv')  # Use your dataset\n",
    "\n",
    "# Data Preprocessing\n",
    "df['Dt'] = pd.to_datetime(df['Dt'], format='%Y-%m-%dT%H:%M:%S.%fZ', errors='coerce')\n",
    "\n",
    "# Drop rows with invalid dates if needed\n",
    "df = df.dropna(subset=['Dt'])\n",
    "\n",
    "# Set the index to the date column\n",
    "df.set_index('Dt', inplace=True)\n",
    "\n",
    "# Aggregating the Transaction Count at Hourly Level\n",
    "hourly_transaction = df.resample('H').size()  # Hourly transaction count\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(hourly_transaction.values.reshape(-1, 1))\n",
    "\n",
    "# Hyperparameters\n",
    "LOOKBACK = 24  # Backcast horizon (input sequence length)\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.00001\n",
    "DROPOUT_RATE = 0.2  # Dropout rate\n",
    "\n",
    "# Prepare the data for training\n",
    "def create_sequences(data, lookback):\n",
    "    x, y = [], []\n",
    "    for i in range(len(data) - lookback):\n",
    "        x.append(data[i : i + lookback])\n",
    "        y.append(data[i + lookback])  # Predicting the next hour\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "# Split the dataset into training and testing\n",
    "split_idx = int(len(scaled_data) * 0.8)\n",
    "train_series, test_series = scaled_data[:split_idx], scaled_data[split_idx:]\n",
    "\n",
    "# Create sequences for training and testing\n",
    "X_train, y_train = create_sequences(train_series, LOOKBACK)\n",
    "X_test, y_test = create_sequences(test_series, LOOKBACK)\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Updated N-BEATS Model Definition with Dropout\n",
    "class NBeatsModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim=256, dropout_rate=0.2):\n",
    "        super(NBeatsModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)  # Dropout layer after first hidden layer\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)  # Dropout layer after second hidden layer\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)  # Additional hidden layer\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)  # Dropout layer after third hidden layer\n",
    "        self.fc4 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input: [batch_size, LOOKBACK] \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)  # Apply dropout\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)  # Apply dropout\n",
    "        x = torch.relu(self.fc3(x))  # Apply ReLU activation to the additional hidden layer\n",
    "        x = self.dropout3(x)  # Apply dropout\n",
    "        forecast = self.fc4(x)\n",
    "        return forecast\n",
    "\n",
    "# Model, loss function, and optimizer with weight decay\n",
    "nbeats_model = NBeatsModel(input_size=LOOKBACK)\n",
    "loss_fn = nn.MSELoss()\n",
    "weight_decay = 0.0001  # L2 regularization factor\n",
    "optimizer = optim.Adam(nbeats_model.parameters(), lr=LEARNING_RATE, weight_decay=weight_decay)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    nbeats_model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        forecast = nbeats_model(X_batch)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = loss_fn(forecast, y_batch.view(-1, 1))\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}/{EPOCHS}, Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "# Testing the model\n",
    "nbeats_model.eval()\n",
    "test_loss = 0.0\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        forecast = nbeats_model(X_batch)\n",
    "        test_loss += loss_fn(forecast, y_batch.view(-1, 1)).item()\n",
    "        predictions.append(forecast)\n",
    "\n",
    "# Calculate the test loss\n",
    "print(f'Test Loss: {test_loss/len(test_loader)}')\n",
    "\n",
    "# Convert predictions back to original scale\n",
    "predictions = torch.cat(predictions, dim=0).cpu().tolist()\n",
    "predictions_rescaled = scaler.inverse_transform(predictions)\n",
    "\n",
    "# Actual values in the test set\n",
    "actual_rescaled = scaler.inverse_transform(test_series[-len(predictions_rescaled):])\n",
    "\n",
    "# Create a DataFrame to store test predictions with timestamps\n",
    "test_timestamps = hourly_transaction.index[-len(predictions_rescaled):]  # Get timestamps from the test set\n",
    "test_predictions_df = pd.DataFrame({\n",
    "    'Dt': test_timestamps,  # Timestamps\n",
    "    'Predicted_Transaction_Count': predictions_rescaled.flatten()  # Forecasted values\n",
    "})\n",
    "\n",
    "# Save the test predictions to CSV\n",
    "test_predictions_path = r'C:\\Users\\admin\\Desktop\\airline\\metro ridership project\\metro_ridership_prediction\\output of sensors\\test_predictions.csv'\n",
    "test_predictions_df.to_csv(test_predictions_path, index=False)\n",
    "print(f\"Test predictions saved to {test_predictions_path}\")\n",
    "\n",
    "# Plotting the forecast vs actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(hourly_transaction.index[-len(predictions_rescaled):], predictions_rescaled, label='N-BEATS Forecast')\n",
    "plt.plot(hourly_transaction.index[-len(actual_rescaled):], actual_rescaled, label='Actual Transaction Count')\n",
    "plt.title(\"N-BEATS Forecast vs Actual Transaction Count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(actual_rescaled, predictions_rescaled)\n",
    "mae = mean_absolute_error(actual_rescaled, predictions_rescaled)\n",
    "r2 = r2_score(actual_rescaled, predictions_rescaled)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"R-Squared: {r2}\")\n",
    "\n",
    "# Save the trained model\n",
    "model_path = r'C:\\Users\\admin\\Desktop\\airline\\metro ridership project\\metro_ridership_prediction\\output of sensors\\nbeats_model.pth'\n",
    "torch.save(nbeats_model.state_dict(), model_path)\n",
    "print(f'Model saved to {model_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf66a359-52f5-4fc8-8b5f-b2891aed4a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "import seaborn as sns\n",
    "\n",
    "# Define a threshold (e.g., mean of actual values)\n",
    "threshold = np.mean(actual_rescaled)\n",
    "\n",
    "# Generate binary labels based on the threshold\n",
    "y_true = (actual_rescaled > threshold).astype(int)\n",
    "y_pred = (predictions_rescaled > threshold).astype(int)\n",
    "\n",
    "# Create confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Calculate accuracy, error rate, and F1 score\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "error_rate = 1 - accuracy\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "# Display results\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Error Rate: {error_rate:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted Neg', 'Predicted Pos'], yticklabels=['Actual Neg', 'Actual Pos'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b53c64e-ed03-4e91-bea4-9ac363cc2e31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30949c2e-901f-477b-be8f-764f46876c29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
