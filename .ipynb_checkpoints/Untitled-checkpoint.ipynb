{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688c11e8-edc9-4f4a-b1d8-5c756be6109a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_14964\\778860528.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  nbeats_model.load_state_dict(torch.load(model_path))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Define N-BEATS Model class (Same as the model used during training)\n",
    "class NBeatsModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim=256, dropout_rate=0.2):\n",
    "        super(NBeatsModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "        self.fc4 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        forecast = self.fc4(x)\n",
    "        return forecast\n",
    "\n",
    "# Load the saved model\n",
    "model_path = r'C:\\Users\\admin\\Desktop\\airline\\metro ridership project\\metro_ridership_prediction\\output of sensors\\nbeats_model.pth'\n",
    "LOOKBACK = 24  # Same as used during training\n",
    "\n",
    "# Initialize and load the model\n",
    "nbeats_model = NBeatsModel(input_size=LOOKBACK)\n",
    "nbeats_model.load_state_dict(torch.load(model_path))\n",
    "nbeats_model.eval()\n",
    "\n",
    "# Feature scaler (reused from training)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Function to get user input for future dates\n",
    "def get_future_dates():\n",
    "    start_date = input(\"Enter start date (YYYY-MM-DD HH:MM:SS): \")\n",
    "    end_date = input(\"Enter end date (YYYY-MM-DD HH:MM:SS): \")\n",
    "\n",
    "    start_datetime = pd.to_datetime(start_date)\n",
    "    end_datetime = pd.to_datetime(end_date)\n",
    "\n",
    "    # Generate hourly timestamps between the start and end date\n",
    "    future_dates = pd.date_range(start=start_datetime, end=end_datetime, freq='H')\n",
    "    return future_dates\n",
    "\n",
    "# Function to make predictions for future dates\n",
    "def predict_future(model, hourly_transaction, future_dates, lookback=LOOKBACK):\n",
    "    # Scale the transaction data using the scaler\n",
    "    scaled_data = scaler.fit_transform(hourly_transaction.values.reshape(-1, 1))\n",
    "    \n",
    "    # Initialize input with the last known data from the dataset\n",
    "    last_known_data = scaled_data[-lookback:].reshape(1, lookback)  # Shape: (1, LOOKBACK)\n",
    "    \n",
    "    predictions = []\n",
    "    for _ in future_dates:\n",
    "        # Convert the last known data to a torch tensor\n",
    "        input_tensor = torch.tensor(last_known_data, dtype=torch.float32)\n",
    "        \n",
    "        # Predict the next hour's transaction count\n",
    "        with torch.no_grad():\n",
    "            forecast = model(input_tensor)\n",
    "        \n",
    "        # Store the prediction\n",
    "        predictions.append(forecast.item())\n",
    "        \n",
    "        # Update the input by appending the predicted value and removing the oldest value\n",
    "        last_known_data = np.append(last_known_data[:, 1:], forecast.item()).reshape(1, lookback)\n",
    "    \n",
    "    # Rescale the predictions back to original scale\n",
    "    predictions_rescaled = scaler.inverse_transform(np.array(predictions).reshape(-1, 1))\n",
    "    return predictions_rescaled\n",
    "\n",
    "# Load the past transaction data (already preprocessed in your existing code)\n",
    "df = pd.read_csv('AFC.3_74_objEvent.csv')  # Load your dataset\n",
    "df['Dt'] = pd.to_datetime(df['Dt'], format='%Y-%m-%dT%H:%M:%S.%fZ', errors='coerce')\n",
    "df = df.dropna(subset=['Dt'])\n",
    "df.set_index('Dt', inplace=True)\n",
    "hourly_transaction = df.resample('H').size()\n",
    "\n",
    "# Get future dates from user input\n",
    "future_dates = get_future_dates()\n",
    "\n",
    "# Make predictions for future dates\n",
    "future_predictions = predict_future(nbeats_model, hourly_transaction, future_dates)\n",
    "\n",
    "# Create a DataFrame to store future predictions\n",
    "future_predictions_df = pd.DataFrame({\n",
    "    'Dt': future_dates,\n",
    "    'Predicted_Transaction_Count': future_predictions.flatten()\n",
    "})\n",
    "\n",
    "# Save future predictions to a CSV file\n",
    "future_predictions_path = r'C:\\Users\\admin\\Desktop\\airline\\metro ridership project\\metro_ridership_prediction\\output of sensors\\future_predictions.csv'\n",
    "future_predictions_df.to_csv(future_predictions_path, index=False)\n",
    "print(f\"Future predictions saved to {future_predictions_path}\")\n",
    "\n",
    "# Plot the future predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(future_dates, future_predictions, label='Predicted Transaction Count')\n",
    "plt.title(\"N-BEATS Future Transaction Count Forecast\")\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Transaction Count')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05d4d67-76cd-430b-b20b-837df78303de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
