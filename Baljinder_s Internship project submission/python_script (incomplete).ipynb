{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WU9ibmMBcVWD","executionInfo":{"status":"ok","timestamp":1730917781314,"user_tz":-330,"elapsed":20636,"user":{"displayName":"Baljinder Singh","userId":"04817123786740350310"}},"outputId":"9c229f24-73e8-468a-ec3c-caa0ee235bce"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install pymongo"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"66Aum3xljLwC","executionInfo":{"status":"ok","timestamp":1730830727962,"user_tz":-330,"elapsed":7236,"user":{"displayName":"Rajesh Kaur","userId":"04744088782592624051"}},"outputId":"b03017b7-2677-4dc8-e479-f7ce2bef5c84","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pymongo\n","  Downloading pymongo-4.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n","Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n","  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n","Downloading pymongo-4.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: dnspython, pymongo\n","Successfully installed dnspython-2.7.0 pymongo-4.10.1\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pXDLhDCFi8Lj"},"outputs":[],"source":["# from pymongo import MongoClient\n","# import pandas as pd\n","\n","# def fetch_data_from_mongo(db_name, collection_name, uri=\"mongodb://localhost:27017/\"):\n","#     \"\"\"\n","#     Connect to a local MongoDB instance, retrieve data from the specified collection,\n","#     and convert it to a DataFrame.\n","\n","#     Parameters:\n","#         db_name (str): Name of the MongoDB database.\n","#         collection_name (str): Name of the collection within the database.\n","#         uri (str): MongoDB connection URI, default is \"mongodb://localhost:27017/\".\n","\n","#     Returns:\n","#         DataFrame: Data from the collection in pandas DataFrame format, or None if no data is available.\n","#     \"\"\"\n","#     # Connect to MongoDB\n","#     client = MongoClient(uri)\n","#     db = client[db_name]\n","#     collection = db[collection_name]\n","\n","#     # Check if any data is available\n","#     if collection.count_documents({}) > 0:\n","#         print(\"Data Available\")\n","#         # Read data from the collection\n","#         documents = collection.find()\n","\n","#         # Convert to DataFrame\n","#         df = pd.DataFrame(documents)\n","#         print(\"Converted to DataFrame Successfully\")\n","#         return df\n","#     else:\n","#         print(\"No Data Available\")\n","#         return None\n","\n","# # Example usage\n","# df = fetch_data_from_mongo(db_name=\"metro_data\", collection_name=\"7_211ObjEvent\")\n","\n","# if df is not None:\n","#     print(df.head())  # Display the first few rows of the DataFrame\n"]},{"cell_type":"code","source":["import pandas as pd\n","\n","def load_and_process_data(path):\n","\n","    # Load the data\n","    df = pd.read_csv(path)\n","    print(\"Data has been loaded successfully\")\n","\n","    # Ensure 'Dt' is in datetime format with timezone handling\n","    df['Dt'] = pd.to_datetime(df['Dt'], utc=True, errors='coerce')\n","\n","    # Extract desired time components\n","    df['hour'] = df['Dt'].dt.hour\n","    df['day'] = df['Dt'].dt.day\n","    df['month'] = df['Dt'].dt.month\n","    df['year'] = df['Dt'].dt.year\n","    df['day_of_week'] = df['Dt'].dt.dayofweek\n","    df['is_weekend'] = df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n","\n","    print(\"Date converted successfully\")\n","    return df\n","path = r\"/content/drive/MyDrive/python_script/7_211_objEvent.csv\"\n","df = load_and_process_data(path)\n","\n"],"metadata":{"id":"Nu6l0qeXjJ-D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730906704385,"user_tz":-330,"elapsed":234224,"user":{"displayName":"Baljinder Singh","userId":"04817123786740350310"}},"outputId":"71ba4b86-89be-4da4-dc47-edf2f7d01153"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Data has been loaded successfully\n","Date converted successfully\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","def data_preprocessing(df):\n","\n","    # Check for NaN values\n","    missing_data = df.isnull().sum()\n","    print(\"Missing values in each column before dropping nulls:\\n\", missing_data)\n","\n","    # Drop rows with any NaN values\n","    df_cleaned = df.dropna()\n","\n","    # Confirm removal of null values\n","    print(\"Data after dropping null values.\")\n","    print(\"Missing values in each column after dropping nulls:\\n\", df_cleaned.isnull().sum())\n","\n","    return df_cleaned\n","\n","df_cleaned = data_preprocessing(df)"],"metadata":{"id":"qiT4ytxlt0F9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730906731225,"user_tz":-330,"elapsed":26845,"user":{"displayName":"Baljinder Singh","userId":"04817123786740350310"}},"outputId":"3104a0be-5eef-439c-8e3d-a331066e995d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Missing values in each column before dropping nulls:\n"," BD             0\n","DSM            0\n","DSM2           0\n","Dt             0\n","ESN            0\n","EqN            0\n","EqT            0\n","EsT            0\n","Line           0\n","Lvl            0\n","Reference      0\n","St             0\n","Sta            0\n","Tag            0\n","_id            0\n","hour           0\n","day            0\n","month          0\n","year           0\n","day_of_week    0\n","is_weekend     0\n","dtype: int64\n","Data after dropping null values.\n","Missing values in each column after dropping nulls:\n"," BD             0\n","DSM            0\n","DSM2           0\n","Dt             0\n","ESN            0\n","EqN            0\n","EqT            0\n","EsT            0\n","Line           0\n","Lvl            0\n","Reference      0\n","St             0\n","Sta            0\n","Tag            0\n","_id            0\n","hour           0\n","day            0\n","month          0\n","year           0\n","day_of_week    0\n","is_weekend     0\n","dtype: int64\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from datetime import datetime, timedelta\n","from statsmodels.tsa.seasonal import seasonal_decompose\n","\n","# 1. Function to load dataset in chunks with the given time format\n","def load_data_in_chunks(file_path, chunk_size=100000, time_format='%Y-%m-%dT%H:%M:%S.%fZ'):\n","    chunks = pd.read_csv(file_path, chunksize=chunk_size, parse_dates=['Dt'], date_parser=lambda x: datetime.strptime(x, time_format))\n","    return chunks\n","\n","# 2. Filter out data till yesterday\n","def filter_data_up_to_yesterday(df):\n","    yesterday = datetime.now() - timedelta(1)  # Yesterday's date\n","    return df[df['Dt'] < yesterday]\n","\n","# 3. Group by 'Sta' and count maximum entries\n","def get_max_sta(df):\n","    sta_count = df.groupby('Sta').size()\n","    max_sta = sta_count.idxmax()  # Station with the max entries\n","    return max_sta, sta_count[max_sta]\n","\n","# 4. Save CSV based on the station with maximum entries\n","def save_csv_for_max_sta(df, max_sta):\n","    max_sta_data = df[df['Sta'] == max_sta]\n","    file_name = f\"{max_sta}.csv\"\n","    max_sta_data.to_csv(file_name, index=False)\n","    return file_name\n","\n","# 5. Find missing date ranges\n","def find_missing_dates(df, freq='D'):\n","    df_sorted = df.sort_values('Dt')\n","    full_range = pd.date_range(start=df_sorted['Dt'].min(), end=df_sorted['Dt'].max(), freq=freq)\n","    missing_dates = full_range.difference(df_sorted['Dt'])\n","    return missing_dates\n","\n","# 6. Delete data before large missing date gap\n","def remove_data_before_large_gap(df, threshold=3):\n","    missing_dates = find_missing_dates(df)\n","    if len(missing_dates) > threshold:\n","        first_missing_date = missing_dates[0]\n","        df = df[df['Dt'] >= first_missing_date]\n","    return df\n","\n","# 7. Handle small missing date gaps using seasonal decomposition\n","def fill_missing_using_seasonal_decomposition(df):\n","    df.set_index('Dt', inplace=True)\n","    result = seasonal_decompose(df['Sta'], model='additive', period=365)  # Assuming yearly seasonality\n","    df['Sta'] = df['Sta'].fillna(result.trend + result.seasonal)\n","    return df\n","\n","# 8. Display first 10 entries of the saved CSV\n","def display_first_10_entries(file_name):\n","    df = pd.read_csv(file_name)\n","    return df.head(10)\n","\n","# Main process to execute the steps\n","def process_ridership_data(file_path):\n","    # Load data in chunks\n","    chunks = load_data_in_chunks(file_path)\n","\n","    # Combine all chunks into a single DataFrame\n","    df_combined = pd.concat(chunks, ignore_index=True)\n","\n","    # Filter data to include only up to yesterday\n","    df_filtered = filter_data_up_to_yesterday(df_combined)\n","\n","    # Get the station with the maximum entries\n","    max_sta, max_sta_count = get_max_sta(df_filtered)\n","\n","    # Save CSV for the max sta\n","    file_name = save_csv_for_max_sta(df_filtered, max_sta)\n","\n","    # Handle missing date ranges (Thresholds and seasonal decomposition)\n","    df_filtered = remove_data_before_large_gap(df_filtered, threshold=3)###################################################################increase threshold to 180\n","    df_filtered = fill_missing_using_seasonal_decomposition(df_filtered)\n","\n","    # Display the first 10 entries of the saved CSV\n","    first_10_entries = display_first_10_entries(file_name)\n","\n","    return first_10_entries\n","\n","# Call the function with your file path\n","file_path = r'/content/drive/MyDrive/python_script/7_211_objEvent.csv'\n","first_10 = process_ridership_data(file_path)\n","print(first_10)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wqh2MRxedknO","outputId":"86079e67-7e36-4b22-f7ae-c42aca837225"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-3-7b1068cc6bf7>:8: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n","  chunks = pd.read_csv(file_path, chunksize=chunk_size, parse_dates=['Dt'], date_parser=lambda x: datetime.strptime(x, time_format))\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"jb8STTZ_I_7p"},"execution_count":null,"outputs":[]}]}